{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype: iota\n",
    "\n",
    "| Properties      | Data    |\n",
    "|---------------|-----------|\n",
    "| *Labels* | `['BENIGN', 'DDoS']` |\n",
    "| *Normalization* | `Min-Max` |\n",
    "| *Sample Size* | `10.000`|\n",
    "| *Adversarial Attack* | `FGSM & C&W` |\n",
    "| *Explanations* | `SHAP` |\n",
    "| *Detector* | `Detect misclassified Samples of both Attacks` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Has to be run first alone!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To import modules from the functions directory\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Building CICIDS2017 dataset --\n",
      "--- Combining all CICIDS2017 files ---\n",
      "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "Wednesday-workingHours.pcap_ISCX.csv\n",
      "Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "Monday-WorkingHours.pcap_ISCX.csv\n",
      "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "--- Removing NaN and Infinity values ---\n",
      "Removing 1358 Rows with NaN values\n",
      "Removing 1509 Rows with Infinity values\n",
      "--- Extracting labels ---\n",
      " Label\n",
      "BENIGN    2271320\n",
      "DDoS       128025\n",
      "Name: count, dtype: int64\n",
      "-- Generating normalizer --\n",
      "--- Splitting labels and features ---\n",
      "Zero Columns: [' Bwd PSH Flags', ' Bwd URG Flags', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate']\n",
      "-- Preprocessing data --\n",
      "--- Sampling balanced data ---\n",
      "Sample to shape: (4000, 79)\n",
      "--- Splitting labels and features ---\n",
      "--- Encoding labels as binary one-hot values ---\n",
      "--- Normalizing features using MinMaxScaler ---\n",
      "Generate Features | Indices: Index([1787989, 978677, 500719, 1942857, 2150382], dtype='int64')... | Shape: (4000, 70)\n",
      "Generate Labels | Indices: Index([1787989, 978677, 500719, 1942857, 2150382], dtype='int64')... | Shape: (4000, 2)\n",
      "BENIGN  ATTACK\n",
      "False   True      2000\n",
      "True    False     2000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import functions.data_preprocessing as dp\n",
    "import importlib\n",
    "importlib.reload(dp)\n",
    "\n",
    "encoding_type = 0 # binary encoding\n",
    "norm_type = 0 # min-max normalization\n",
    "label_names = ['BENIGN', 'DDoS'] # labels to include\n",
    "sample_size = 2000 # sample size for each label -> 2 x sample_size = total samples\n",
    "\n",
    "dataset = dp.build_dataset(label_names)\n",
    "\n",
    "normalizer, zero_columns = dp.generate_normalizer(dataset, norm_type)\n",
    "\n",
    "feature_df, label_df, used_indices = dp.preprocess_data(dataset, encoding_type, normalizer, zero_columns, sample_size=sample_size, random_sample_state=42)\n",
    "print(f\"Generate Features | Indices: {feature_df.index[:5]}... | Shape: {feature_df.shape}\")\n",
    "print(f\"Generate Labels | Indices: {label_df.index[:5]}... | Shape: {label_df.shape}\")\n",
    "print(label_df.value_counts()) # -> will first show [0, 1] then [1, 0] if label number is equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 70) (800, 70) (3200, 2) (800, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, label_df, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 13:38:38.461027: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742992718.472483  104057 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742992718.475963  104057 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-26 13:38:38.488721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 13:38:39.838813: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-03-26 13:38:39.908331: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7456 - loss: 0.6516 - val_accuracy: 0.8609 - val_loss: 0.5484\n",
      "Epoch 2/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8855 - loss: 0.5086 - val_accuracy: 0.9438 - val_loss: 0.3615\n",
      "Epoch 3/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9592 - loss: 0.3196 - val_accuracy: 0.9781 - val_loss: 0.1820\n",
      "Epoch 4/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9664 - loss: 0.1651 - val_accuracy: 0.9781 - val_loss: 0.0931\n",
      "Epoch 5/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9689 - loss: 0.1022 - val_accuracy: 0.9781 - val_loss: 0.0696\n",
      "Epoch 6/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9699 - loss: 0.0867 - val_accuracy: 0.9781 - val_loss: 0.0616\n",
      "Epoch 7/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9711 - loss: 0.0806 - val_accuracy: 0.9781 - val_loss: 0.0575\n",
      "Epoch 8/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9715 - loss: 0.0771 - val_accuracy: 0.9781 - val_loss: 0.0546\n",
      "Epoch 9/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9716 - loss: 0.0750 - val_accuracy: 0.9781 - val_loss: 0.0526\n",
      "Epoch 10/10\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9719 - loss: 0.0732 - val_accuracy: 0.9781 - val_loss: 0.0506\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step\n",
      "Global Accuracy: 97.62%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ATTACK       0.95      1.00      0.98       378\n",
      "      BENIGN       1.00      0.95      0.98       422\n",
      "\n",
      "    accuracy                           0.98       800\n",
      "   macro avg       0.98      0.98      0.98       800\n",
      "weighted avg       0.98      0.98      0.98       800\n",
      "\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402us/step\n",
      "Predictions on Normal Data | Indices: Index([423092, 983287, 359890, 2216101, 2576515], dtype='int64')... | Shape: (3200, 2)\n"
     ]
    }
   ],
   "source": [
    "import functions.intrusion_detection_system as ids\n",
    "import importlib\n",
    "importlib.reload(ids)\n",
    "\n",
    "# TODO: build ids with complete dataset\n",
    "# X_train_all, y_train_all, _ = dp.preprocess_data(dataset, encoding_type, normalizer, zero_columns, random_sample_state=42)\n",
    "# print(y_train_all.value_counts())\n",
    "# X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=42)\n",
    "# print(X_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape)\n",
    "\n",
    "# build ids and evaluate it on test data\n",
    "ids_model = ids.build_intrusion_detection_system(X_train, y_train, X_test, y_test)\n",
    "# store prediction from X_train\n",
    "y_pred = ids.predict(ids_model, X_train, columns=y_train.columns)\n",
    "print(f\"Predictions on Normal Data | Indices: {y_pred.index[:5]}... | Shape: {y_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate C&W and FGSM Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 70) (3168, 70) (32, 2) (3168, 2)\n",
      "Running attack using 24 CPU cores...\n",
      "\n",
      "Process 105453 is generating adversarial examples for batch of size 132 \n",
      "Process 105454 is generating adversarial examples for batch of size 132 \n",
      "Process 105455 is generating adversarial examples for batch of size 132 \n",
      "Process 105457 is generating adversarial examples for batch of size 132 \n",
      "Process 105458 is generating adversarial examples for batch of size 132 \n",
      "Process 105456 is generating adversarial examples for batch of size 132 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Process 105459 is generating adversarial examples for batch of size 132 \n",
      "\n",
      "\n",
      "Process 105460 is generating adversarial examples for batch of size 132 \n",
      "Process 105461 is generating adversarial examples for batch of size 132 \n",
      "Process 105462 is generating adversarial examples for batch of size 132 \n",
      "\n",
      "Process 105463 is generating adversarial examples for batch of size 132 \n",
      "Process 105464 is generating adversarial examples for batch of size 132 \n",
      "\n",
      "\n",
      "Process 105465 is generating adversarial examples for batch of size 132 \n",
      "\n",
      "Process 105466 is generating adversarial examples for batch of size 132 \n",
      "Process 105467 is generating adversarial examples for batch of size 132 \n",
      "Process 105468 is generating adversarial examples for batch of size 132 \n",
      "\n",
      "Process 105469 is generating adversarial examples for batch of size 132 \n",
      "\n",
      "Process 105470 is generating adversarial examples for batch of size 132 \n",
      "Process 105471 is generating adversarial examples for batch of size 132 \n",
      "Process 105472 is generating adversarial examples for batch of size 132 \n",
      "Process 105474 is generating adversarial examples for batch of size 132 \n",
      "Process 105475 is generating adversarial examples for batch of size 132 \n",
      "Process 105476 is generating adversarial examples for batch of size 132 \n",
      "Process 105473 is generating adversarial examples for batch of size 132 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7452c7f6c07841eb991249743b3224f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e05abec95742d893155344df9f6811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5384fe4c724bbd9e3c979b013d3067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ea41b9575c4009aa51f7f780cf05cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3facae0394ac42d1a1bda7390764f9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73b038c3d344db7a21e118d3b266cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b4fffa431a4443afb21b1e082779ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e04abd6dc5d4f7ab9e0bceffa476277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064096e3e33141bd9e62f5c6b140a9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dffacbac176b447cbb0c5c7f6f69e2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1b22b42c704b1aa17665ece310f651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7d1cd16c334c2da93f69ce6490d197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ec3700f5c14127a7e4effd83189e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b7aceb6e4e4f14ac3f440c0295cfd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d16aa4cd2ab48b99e5b156ca0905ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14947be5474349938bb23fcfd89d60bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515fa006e94942858127654bfb7e51f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ee282e85864c0495ec78881628346c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdfce53e3b547d98a1a05fa9105804e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abffc7255d7c4b4f99d7011d5e9eaebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de32fa936ae74c9bb7b6dd35c561aa05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d870ea4cb6c40cc84bb4ba913448794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8f5303d71645098518d601efe3c3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdbbb467052a4cb08a5d630c95dddd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Adversarial Attack | Indices: Index([298341, 2039933, 747143, 476707, 2104689], dtype='int64')... | Shape: (3168, 70)\n",
      "Accuracy: 71.53%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ATTACK       1.00      0.44      0.61      1607\n",
      "      BENIGN       0.63      1.00      0.78      1561\n",
      "\n",
      "    accuracy                           0.72      3168\n",
      "   macro avg       0.82      0.72      0.69      3168\n",
      "weighted avg       0.82      0.72      0.69      3168\n",
      "\n",
      "Confusion Matrix: Positive == BENIGN\n",
      "TN: 705, FP: 902, FN: 0, TP: 1561\n",
      "Predictions on Adversarial Attacks | Indices: Index([298341, 2039933, 747143, 476707, 2104689], dtype='int64')... | Shape: (3168, 2)\n"
     ]
    }
   ],
   "source": [
    "import functions.attack_generator as ag\n",
    "import importlib\n",
    "import numpy as np\n",
    "importlib.reload(ag)\n",
    "\n",
    "all_features = dataset.drop(columns=[' Label'])\n",
    "art_model = ag.convert_to_art_model(ids_model, X_train) # TODO: use all features for generating art model\n",
    "\n",
    "# split train data into data for generating fgsm and cw attacks\n",
    "X_fgsm, X_cw, y_fgsm, y_cw = train_test_split(X_train, y_train, test_size=0.99, random_state=15)\n",
    "print(X_fgsm.shape, X_cw.shape, y_fgsm.shape, y_cw.shape)\n",
    "\n",
    "# generate attacks on the separated training data\n",
    "# TODO: when changing epsilon, the detector accuracy rises\n",
    "# X_adv_fgsm = ag.generate_fgsm_attacks(art_model, X_train, 1)\n",
    "# print(f\"Create Adversarial Attack | Indices: {X_adv_fgsm.index[:5]}... | Shape: {X_adv_fgsm.shape}\")\n",
    "# y_pred_adv_fgsm = ag.evaluate_art_model(art_model, X_adv_fgsm, y_train)\n",
    "# print(f\"Predictions on Adversarial Attacks | Indices: {y_pred_adv_fgsm.index[:5]}... | Shape: {y_pred_adv_fgsm.shape}\")\n",
    "# y_pred_fgsm = y_pred.loc[X_fgsm.index]\n",
    "\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "X_adv_cw = ag.generate_cw_attacks_parallel(art_model, X_cw, 1, num_cores=num_cores)\n",
    "print(f\"Create Adversarial Attack | Indices: {X_adv_cw.index[:5]}... | Shape: {X_adv_cw.shape}\")\n",
    "y_pred_adv_cw = ag.evaluate_art_model(art_model, X_adv_cw, y_cw)\n",
    "print(f\"Predictions on Adversarial Attacks | Indices: {y_pred_adv_cw.index[:5]}... | Shape: {y_pred_adv_cw.shape}\")\n",
    "y_pred_cw = y_pred.loc[X_cw.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correctly_benign_classified_indices(y_train, y_pred):\n",
    "    benign_indices = y_train[y_train['BENIGN'] == 1].index\n",
    "    benign_adv_predicted_indices = y_pred[y_pred['BENIGN'] == 1].index\n",
    "    correctly_benign_classified_indices = benign_indices.intersection(benign_adv_predicted_indices)\n",
    "    return correctly_benign_classified_indices\n",
    "\n",
    "def get_misclassified_as_benign_due_attack_indices(y_train, y_pred, y_pred_adv):\n",
    "    attack_indices = y_train[y_train['ATTACK'] == 1].index\n",
    "    attack_adv_predicted_indices = y_pred[y_pred['ATTACK'] == 1].index\n",
    "    benign_predicted_adversarial_indices = y_pred_adv[y_pred_adv['BENIGN'] == 1].index\n",
    "    misclassified_as_benign_due_attack_indices = attack_indices.intersection(attack_adv_predicted_indices).intersection(benign_predicted_adversarial_indices)\n",
    "    return misclassified_as_benign_due_attack_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly classified as BENIGN from the IDS: 1508 | Indices: Index([983287, 2216101, 2576515], dtype='int64')\n",
      "Correctly classified as BENIGN from the IDS (CW): 1491 | Indices: Index([298341, 2039933, 747143], dtype='int64')\n",
      "ATTACK sample misclassified as BENIGN due to adversarial attack (CW): 900 | Indices: Index([476707, 448348, 321029], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "correctly_benign_classified_indices = get_correctly_benign_classified_indices(y_train, y_pred)\n",
    "# correctly_benign_classified_indices_fgsm = get_correctly_benign_classified_indices(y_fgsm, y_pred_adv_fgsm)\n",
    "# misclassified_as_benign_due_attack_indices_fgsm = get_misclassified_as_benign_due_attack_indices(y_fgsm, y_pred_fgsm, y_pred_adv_fgsm)\n",
    "correctly_benign_classified_indices_cw = get_correctly_benign_classified_indices(y_cw, y_pred_cw)\n",
    "misclassified_as_benign_due_attack_indices_cw = get_misclassified_as_benign_due_attack_indices(y_cw, y_pred_cw, y_pred_adv_cw)\n",
    "print(f\"Correctly classified as BENIGN from the IDS: {len(correctly_benign_classified_indices)} | Indices: {correctly_benign_classified_indices[:3]}\")\n",
    "# print(f\"Correctly classified as BENIGN from the IDS (FGSM): {len(correctly_benign_classified_indices_fgsm)} | Indices: {correctly_benign_classified_indices_fgsm[:3]}\")\n",
    "# print(f\"ATTACK sample misclassified as BENIGN due to adversarial attack (FGSM): {len(misclassified_as_benign_due_attack_indices_fgsm)} | Indices: {misclassified_as_benign_due_attack_indices_fgsm[:3]}\")\n",
    "print(f\"Correctly classified as BENIGN from the IDS (CW): {len(correctly_benign_classified_indices_cw)} | Indices: {correctly_benign_classified_indices_cw[:3]}\")\n",
    "print(f\"ATTACK sample misclassified as BENIGN due to adversarial attack (CW): {len(misclassified_as_benign_due_attack_indices_cw)} | Indices: {misclassified_as_benign_due_attack_indices_cw[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions.visualizer as visualizer\n",
    "# import importlib\n",
    "# importlib.reload(visualizer)\n",
    "\n",
    "# visualizer.visualize_data_distribution(X_train.loc[correctly_benign_classified_indices], 'Normal Data', X_adv_fgsm.loc[misclassified_as_benign_due_attack_indices_fgsm], 'Adversarial Data')\n",
    "# # visualizer.pca_visualization_side_by_side(X_train.loc[misclassified_as_benign_due_attack_indices], 'Normal Data', X_adv.loc[misclassified_as_benign_due_attack_indices], 'Adversarial Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 3201it [01:41, 29.28it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Explanations | Indices: Index([423092, 983287, 359890, 2216101, 2576515], dtype='int64')... | Shape: (3200, 70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 3169it [01:39, 29.06it/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Adversarial Explanations | Indices: Index([298341, 2039933, 747143, 476707, 2104689], dtype='int64')... | Shape: (3168, 70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import functions.explainer as exp\n",
    "import importlib\n",
    "importlib.reload(exp)\n",
    "\n",
    "explainer = exp.generate_shap_explainer(ids_model, X_train)\n",
    "\n",
    "shap_values, shap_values_df = exp.generate_shap_values(explainer, X_train)\n",
    "print(f\"Generate Explanations | Indices: {shap_values_df.index[:5]}... | Shape: {shap_values_df.shape}\")\n",
    "\n",
    "# _, shap_values_adv_df_fgsm = exp.generate_shap_values(explainer, X_adv_fgsm)\n",
    "# print(f\"Generate Adversarial Explanations | Indices: {shap_values_adv_df_fgsm.index[:5]}... | Shape: {shap_values_adv_df_fgsm.shape}\")\n",
    "\n",
    "_, shap_values_adv_df_cw = exp.generate_shap_values(explainer, X_adv_cw)\n",
    "print(f\"Generate Adversarial Explanations | Indices: {shap_values_adv_df_cw.index[:5]}... | Shape: {shap_values_adv_df_cw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # concat_correctly_benign_classified_shaps = pd.concat([shap_values_df.loc[correctly_benign_classified_indices], shap_values_adv_df_fgsm.loc[correctly_benign_classified_indices_fgsm], shap_values_adv_df_cw.loc[correctly_benign_classified_indices_cw]], axis=0)\n",
    "# # # shap_values_df.loc[misclassified_as_benign_due_attack_indices]\n",
    "# # concat_misclassified_as_benign_shaps = pd.concat([shap_values_adv_df_fgsm.loc[misclassified_as_benign_due_attack_indices_fgsm], shap_values_adv_df_cw.loc[misclassified_as_benign_due_attack_indices_cw]], axis=0)\n",
    "\n",
    "# concat_correctly_benign_classified_shaps = pd.concat([shap_values_df.loc[correctly_benign_classified_indices], shap_values_adv_df_fgsm.loc[correctly_benign_classified_indices_fgsm]], axis=0)\n",
    "# # shap_values_df.loc[misclassified_as_benign_due_attack_indices]\n",
    "# concat_misclassified_as_benign_shaps = pd.concat([shap_values_adv_df_fgsm.loc[misclassified_as_benign_due_attack_indices_fgsm]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions.visualizer as visualizer\n",
    "# import importlib\n",
    "# importlib.reload(visualizer)\n",
    "\n",
    "# visualizer.visualize_data_distribution(shap_values_df.loc[correctly_benign_classified_indices], 'Normal Explanations', shap_values_adv_df_fgsm.loc[misclassified_as_benign_due_attack_indices_fgsm], 'Adversarial Explanations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp.plot_shap_summary_comparison(shap_values_df.loc[correctly_benign_classified_indices].values, X_train.loc[correctly_benign_classified_indices], shap_values_adv_df.loc[misclassified_as_benign_due_attack_indices].values, X_adv.loc[misclassified_as_benign_due_attack_indices], 6, title='Normal vs Adversarial Explanations of Benign Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Correctly Classified as Benign: 1508\n",
      "Normal Misclassified as Benign: 2\n",
      "Adversarial Correctly Classified as Benign (FGSM): 1491\n",
      "Adversarial Misclassified as Benign (FGSM): 900\n"
     ]
    }
   ],
   "source": [
    "adversarial_correct_benign_indices = correctly_benign_classified_indices_cw\n",
    "adversarial_misclassified_as_benign_indices = misclassified_as_benign_due_attack_indices_cw\n",
    "normal_correct_benign_indices = correctly_benign_classified_indices\n",
    "\n",
    "attack_indices = y_train[y_train['ATTACK'] == 1].index\n",
    "predicted_benign_indices = y_pred[y_pred['BENIGN'] == 1].index\n",
    "normal_misclassified_as_benign_indices = attack_indices.intersection(predicted_benign_indices)\n",
    "\n",
    "print(f\"Normal Correctly Classified as Benign: {len(normal_correct_benign_indices)}\")\n",
    "print(f\"Normal Misclassified as Benign: {len(normal_misclassified_as_benign_indices)}\")\n",
    "print(f\"Adversarial Correctly Classified as Benign (FGSM): {len(adversarial_correct_benign_indices)}\")\n",
    "print(f\"Adversarial Misclassified as Benign (FGSM): {len(adversarial_misclassified_as_benign_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1491, 70) (900, 70) (1508, 70) (2, 70)\n",
      "ADV CORRECT BENIGN | ADV MISCLASSIFIED BENIGN | NORM CORRECT BENIGN | NORM MISCLASSIFIED BENIGN\n",
      "(3901, 70) (3901, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "shap_adv_correct_benign = shap_values_adv_df_cw.loc[adversarial_correct_benign_indices]\n",
    "shap_adv_misclassified_benign = shap_values_adv_df_cw.loc[adversarial_misclassified_as_benign_indices]\n",
    "shap_normal_correct_benign = shap_values_df.loc[normal_correct_benign_indices]\n",
    "shap_normal_misclassified_benign = shap_values_df.loc[normal_misclassified_as_benign_indices]\n",
    "print(shap_adv_correct_benign.shape, shap_adv_misclassified_benign.shape, shap_normal_correct_benign.shape, shap_normal_misclassified_benign.shape)\n",
    "print('ADV CORRECT BENIGN |', 'ADV MISCLASSIFIED BENIGN |', 'NORM CORRECT BENIGN |', 'NORM MISCLASSIFIED BENIGN')\n",
    "\n",
    "\n",
    "# build dataset\n",
    "y_adv_benign = np.array([[1, 0, 0, 0]] * shap_adv_correct_benign.shape[0])  \n",
    "y_adv_attack = np.array([[0, 1, 0, 0]] * shap_adv_misclassified_benign.shape[0])\n",
    "y_norm_bening = np.array([[0, 0, 1, 0]] * shap_normal_correct_benign.shape[0])\n",
    "y_norm_attack = np.array([[0, 0, 0, 1]] * shap_normal_misclassified_benign.shape[0])\n",
    "\n",
    "\n",
    "y = np.concatenate([y_adv_benign, y_adv_attack, y_norm_bening, y_norm_attack])\n",
    "y = pd.DataFrame(y, columns=['ADV CORRECT BENIGN', 'ADV MISCLASSIFIED BENIGN', 'NORM CORRECT BENIGN', 'NORM MISCLASSIFIED BENIGN'])\n",
    "\n",
    "X = pd.concat([shap_adv_correct_benign, shap_adv_misclassified_benign, shap_normal_correct_benign, shap_normal_misclassified_benign], axis=0)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3901, 70) (3901, 4)\n",
      "(3510, 70) (391, 70) (3510, 4) (391, 4)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 13:49:22.791374: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2476 - loss: 0.6839 - val_accuracy: 0.3889 - val_loss: 0.6468\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2881 - loss: 0.6350 - val_accuracy: 0.4003 - val_loss: 0.5756\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3934 - loss: 0.5734 - val_accuracy: 0.4074 - val_loss: 0.5151\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4258 - loss: 0.5248 - val_accuracy: 0.5256 - val_loss: 0.4514\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4791 - loss: 0.4719 - val_accuracy: 0.5570 - val_loss: 0.3955\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5329 - loss: 0.4286 - val_accuracy: 0.5855 - val_loss: 0.3603\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5595 - loss: 0.4024 - val_accuracy: 0.5783 - val_loss: 0.3410\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5774 - loss: 0.3830 - val_accuracy: 0.5726 - val_loss: 0.3269\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5823 - loss: 0.3671 - val_accuracy: 0.5855 - val_loss: 0.3148\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5842 - loss: 0.3541 - val_accuracy: 0.5840 - val_loss: 0.3069\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
     ]
    }
   ],
   "source": [
    "import functions.detector as det\n",
    "import importlib\n",
    "importlib.reload(det)\n",
    "\n",
    "# build detector to detect adversarial samples that misclassify attack samples as benign\n",
    "\n",
    "# create dataframe\n",
    "# TODO: build detector with normal and adversarial shap values?\n",
    "# TODO: build with shap_values_adv_df to detect 'BENIGN' and 'ATTACK'\n",
    "import pandas as pd\n",
    "\n",
    "# alternative approach: detector that predicts the original label of the sample for all given adversarial attacks\n",
    "# concat_correctly_benign_classified_shaps = pd.concat([shap_values_df.loc[correctly_benign_classified_indices], shap_values_adv_df.loc[correctly_benign_classified_indices]], axis=0)\n",
    "# concat_misclassified_as_benign_shaps = pd.concat([shap_values_df.loc[misclassified_as_benign_due_attack_indices], shap_values_adv_df.loc[misclassified_as_benign_due_attack_indices]], axis=0)\n",
    "# X, y = det.build_train_datasets(shap_values_df.loc[correctly_benign_classified_indices], shap_values_adv_df_fgsm.loc[misclassified_as_benign_due_attack_indices_fgsm])\n",
    "\n",
    "#X, y = det.build_train_datasets(shap_values_df.loc[correctly_benign_classified_indices], shap_values_adv_df.loc[misclassified_as_benign_due_attack_indices])\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# split data\n",
    "X_train_det, X_test_det, y_train_det, y_test_det = train_test_split(X, y, test_size=0.1, random_state=1503)\n",
    "print(X_train_det.shape, X_test_det.shape, y_train_det.shape, y_test_det.shape)\n",
    "\n",
    "# build detector\n",
    "detector = det.build_detector(X_train_det, y_train_det, X_test_det, y_test_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Predictions on Detector | Indices: Index([2106199, 557208, 2154294, 310980, 1439561], dtype='int64')... | Shape: (391, 4)\n",
      "[0 2 2 1 0] [0 0 0 1 0]\n",
      "Overall Accuracy: 0.5780\n",
      "Classification Report (Overall):\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "      ADV CORRECT BENIGN       0.47      0.99      0.63       143\n",
      "ADV MISCLASSIFIED BENIGN       0.98      1.00      0.99        84\n",
      "     NORM CORRECT BENIGN       0.00      0.00      0.00       164\n",
      "\n",
      "                accuracy                           0.58       391\n",
      "               macro avg       0.48      0.66      0.54       391\n",
      "            weighted avg       0.38      0.58      0.44       391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate detector\n",
    "y_pred_det = det.predict(detector, X_test_det, y.columns)\n",
    "print(f\"Predictions on Detector | Indices: {y_pred_det.index[:5]}... | Shape: {y_pred_det.shape}\")\n",
    "\n",
    "# Convert one-hot to class indices\n",
    "y_true_indices = np.argmax(y_test_det, axis=1)\n",
    "y_pred_indices = np.argmax(y_pred_det, axis=1)\n",
    "print(y_true_indices[:5], y_pred_indices[:5])\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Compute Accuracy\n",
    "accuracy = accuracy_score(y_true_indices, y_pred_indices)\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compute Classification Report for overall classification\n",
    "print(\"Classification Report (Overall):\")\n",
    "print(classification_report(y_true_indices, y_pred_indices, target_names=['ADV CORRECT BENIGN', 'ADV MISCLASSIFIED BENIGN', 'NORM CORRECT BENIGN'], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1] [1 1 1 1 1]\n",
      "[1 1 1 0 1] [1 1 1 0 1]\n",
      "\n",
      "Adversarial Detection (Class 1 + 2):\n",
      "TP (Adversarial correctly detected): 227\n",
      "FP (Benign incorrectly detected as adversarial): 164\n",
      "TN (Benign correctly detected): 0\n",
      "FN (Adversarial missed): 0\n",
      "Adversarial Detection Metrics:\n",
      "Adversarial Detection Accuracy: 58.06%\n",
      "True Positive Rate (TPR): 100.00%\n",
      "False Positive Rate (FPR): 100.00%\n",
      "False Negative Rate (FNR): 0.00%\n",
      "True Negative Rate (TNR): 0.00%\n",
      "\n",
      "Benign Detection (Class 1 + 3):\n",
      "TP (Benign correctly detected): 305\n",
      "FP (Adversarial incorrectly detected as benign): 0\n",
      "TN (Adversarial correctly detected): 84\n",
      "FN (Benign missed): 2\n",
      "Benign Detection Metrics:\n",
      "Benign Detection Accuracy: 99.49%\n",
      "True Positive Rate (TPR): 99.35%\n",
      "False Positive Rate (FPR): 0.00%\n",
      "False Negative Rate (FNR): 0.65%\n",
      "True Negative Rate (TNR): 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Combine classes for adversarial detection (class 1 and class 2)\n",
    "y_true_adv = np.where(np.isin(y_true_indices, [0, 1]), 1, 0)  # Adversarial = 1 (class 1 or 2), otherwise 0\n",
    "y_pred_adv = np.where(np.isin(y_pred_indices, [0, 1]), 1, 0)  # Predicted as Adversarial\n",
    "print(y_true_adv[:5], y_pred_adv[:5])\n",
    "\n",
    "# Combine classes for benign detection (class 1 and class 3)\n",
    "y_true_benign = np.where(np.isin(y_true_indices, [0, 2]), 1, 0)  # Benign = 1 (class 1 or 3), otherwise 0\n",
    "y_pred_benign = np.where(np.isin(y_pred_indices, [0, 2]), 1, 0)  # Predicted as Benign\n",
    "print(y_true_benign[:5], y_pred_benign[:5])\n",
    "\n",
    "\n",
    "# Compute confusion matrix for Adversarial Detection\n",
    "tn_adv, fp_adv, fn_adv, tp_adv = confusion_matrix(y_true_adv, y_pred_adv).ravel()\n",
    "print(f\"\\nAdversarial Detection (Class 1 + 2):\")\n",
    "print(f\"TP (Adversarial correctly detected): {tp_adv}\")\n",
    "print(f\"FP (Benign incorrectly detected as adversarial): {fp_adv}\")\n",
    "print(f\"TN (Benign correctly detected): {tn_adv}\")\n",
    "print(f\"FN (Adversarial missed): {fn_adv}\")\n",
    "\n",
    "# Calculate metrics for Adversarial Detection\n",
    "tpr_adv = tp_adv / (tp_adv + fn_adv) if (tp_adv + fn_adv) != 0 else 0\n",
    "fpr_adv = fp_adv / (fp_adv + tn_adv) if (fp_adv + tn_adv) != 0 else 0\n",
    "fnr_adv = fn_adv / (tp_adv + fn_adv) if (tp_adv + fn_adv) != 0 else 0\n",
    "tnr_adv = tn_adv / (tn_adv + fp_adv) if (tn_adv + fp_adv) != 0 else 0  \n",
    "\n",
    "# Calculate accuracy for Adversarial Detection\n",
    "accuracy_adv = (tp_adv + tn_adv) / (tp_adv + tn_adv + fp_adv + fn_adv) if (tp_adv + tn_adv + fp_adv + fn_adv) != 0 else 0\n",
    "\n",
    "print(f\"Adversarial Detection Metrics:\")\n",
    "print(f\"Adversarial Detection Accuracy: {100*accuracy_adv:.2f}%\")\n",
    "print(f\"True Positive Rate (TPR): {100*tpr_adv:.2f}%\")\n",
    "print(f\"False Positive Rate (FPR): {100*fpr_adv:.2f}%\")\n",
    "print(f\"False Negative Rate (FNR): {100* fnr_adv:.2f}%\")\n",
    "print(f\"True Negative Rate (TNR): {100*tnr_adv:.2f}%\") \n",
    "\n",
    "# Compute confusion matrix for Benign Detection\n",
    "tn_benign, fp_benign, fn_benign, tp_benign = confusion_matrix(y_true_benign, y_pred_benign).ravel()\n",
    "print(f\"\\nBenign Detection (Class 1 + 3):\")\n",
    "print(f\"TP (Benign correctly detected): {tp_benign}\")\n",
    "print(f\"FP (Adversarial incorrectly detected as benign): {fp_benign}\")\n",
    "print(f\"TN (Adversarial correctly detected): {tn_benign}\")\n",
    "print(f\"FN (Benign missed): {fn_benign}\")\n",
    "\n",
    "# Calculate metrics for Benign Detection\n",
    "tpr_benign = tp_benign / (tp_benign + fn_benign) if (tp_benign + fn_benign) != 0 else 0\n",
    "fpr_benign = fp_benign / (fp_benign + tn_benign) if (fp_benign + tn_benign) != 0 else 0\n",
    "fnr_benign = fn_benign / (tp_benign + fn_benign) if (tp_benign + fn_benign) != 0 else 0\n",
    "tnr_benign = tn_benign / (tn_benign + fp_benign) if (tn_benign + fp_benign) != 0 else 0  \n",
    "\n",
    "# Calculate accuracy for Benign Detection\n",
    "accuracy_benign = (tp_benign + tn_benign) / (tp_benign + tn_benign + fp_benign + fn_benign) if (tp_benign + tn_benign + fp_benign + fn_benign) != 0 else 0\n",
    "\n",
    "print(f\"Benign Detection Metrics:\")\n",
    "print(f\"Benign Detection Accuracy: {100*accuracy_benign:.2f}%\")\n",
    "print(f\"True Positive Rate (TPR): {100*tpr_benign:.2f}%\")\n",
    "print(f\"False Positive Rate (FPR): {100*fpr_benign:.2f}%\")\n",
    "print(f\"False Negative Rate (FNR): {100*fnr_benign:.2f}%\")\n",
    "print(f\"True Negative Rate (TNR): {100*tnr_benign:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Samples: 0 | []\n",
      "Benign Samples: 305 | [0 1 2 4 6]\n",
      "Intersection of Adversarial and Benign Samples: 0\n",
      "Normal Samples: 0 | []\n"
     ]
    }
   ],
   "source": [
    "# find intersection of benign and normal samples\n",
    "normal_samples = np.where(y_pred_adv == 0)[0]\n",
    "print(f\"Normal Samples: {len(normal_samples)} | {normal_samples[:5]}\")\n",
    "normal_samples = set(normal_samples)\n",
    "benign_samples = np.where(y_pred_benign == 1)[0]\n",
    "print(f\"Benign Samples: {len(benign_samples)} | {benign_samples[:5]}\")\n",
    "benign_samples = set(benign_samples)\n",
    "intersection = normal_samples & benign_samples\n",
    "print(f\"Intersection of Adversarial and Benign Samples: {len(intersection)}\")\n",
    "\n",
    "# find incides from class [0, 0, 1, 0] of y_pred_indices\n",
    "normal_indices = np.where(y_pred_indices == 2)[0]\n",
    "print(f\"Normal Samples: {len(normal_indices)} | {normal_indices[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Manual Evaluation\n",
    "We perform the whole two-stages approach on new unseen data and evaluate the following scores:\n",
    "- Recall\n",
    "- Precision\n",
    "- Accuracy\n",
    "- F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Preprocessing data --\n",
      "--- Sampling balanced data ---\n",
      "Sample to shape: (1000, 79)\n",
      "--- Splitting labels and features ---\n",
      "--- Encoding labels as binary one-hot values ---\n",
      "--- Normalizing features using MinMaxScaler ---\n",
      "Generate Features | Indices: Index([2056787, 2391506, 802264, 1981689, 480604], dtype='int64')... | Shape: (1000, 70)\n",
      "Generate Labels | Indices: Index([2056787, 2391506, 802264, 1981689, 480604], dtype='int64')... | Shape: (1000, 2)\n",
      "BENIGN  ATTACK\n",
      "False   True      500\n",
      "True    False     500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import functions.data_preprocessing as dp\n",
    "import importlib\n",
    "importlib.reload(dp)\n",
    "\n",
    "# exclude previously used samples\n",
    "dataset_eval_excluded = dataset.drop(index=used_indices)\n",
    "\n",
    "X_eval, y_eval, used_eval_indices = dp.preprocess_data(dataset_eval_excluded, encoding_type, normalizer, zero_columns, sample_size=500, random_sample_state=17)\n",
    "print(f\"Generate Features | Indices: {X_eval.index[:5]}... | Shape: {X_eval.shape}\")\n",
    "print(f\"Generate Labels | Indices: {y_eval.index[:5]}... | Shape: {y_eval.shape}\")\n",
    "print(y_eval.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial FGSM examples generated. Shape: (1000, 70)\n",
      "Create Adversarial Attack | Indices: Index([2056787, 2391506, 802264, 1981689, 480604], dtype='int64')... | Shape: (1000, 70)\n",
      "Accuracy: 50.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ATTACK       0.00      0.00      0.00       500\n",
      "      BENIGN       0.50      1.00      0.67       500\n",
      "\n",
      "    accuracy                           0.50      1000\n",
      "   macro avg       0.25      0.50      0.33      1000\n",
      "weighted avg       0.25      0.50      0.33      1000\n",
      "\n",
      "Confusion Matrix: Positive == BENIGN\n",
      "TN: 0, FP: 500, FN: 0, TP: 500\n",
      "Predictions on Adversarial Attacks | Indices: Index([2056787, 2391506, 802264, 1981689, 480604], dtype='int64')... | Shape: (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(ag)\n",
    "\n",
    "# X_adv_eval = ag.generate_cw_attacks_parallel(art_model, X_eval, target_label=1, num_cores=num_cores)\n",
    "# print(f\"Create Adversarial Attack | Indices: {X_adv_eval.index[:5]}... | Shape: {X_adv_eval.shape}\")\n",
    "\n",
    "X_adv_eval = ag.generate_fgsm_attacks(art_model, X_eval, target_label=1)\n",
    "print(f\"Create Adversarial Attack | Indices: {X_adv_eval.index[:5]}... | Shape: {X_adv_eval.shape}\")\n",
    "\n",
    "y_pred_adv_eval = ag.evaluate_art_model(art_model, X_adv_eval, y_eval)\n",
    "print(f\"Predictions on Adversarial Attacks | Indices: {y_pred_adv_eval.index[:5]}... | Shape: {y_pred_adv_eval.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 1001it [00:29, 22.27it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Explanations | Indices: Index([2056787, 2391506, 802264, 1981689, 480604], dtype='int64')... | Shape: (1000, 70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(exp)\n",
    "X_eval_adv_shap_values, X_eval_adv_shap_values_df = exp.generate_shap_values(explainer, X_adv_eval)\n",
    "\n",
    "print(f\"Create Explanations | Indices: {X_eval_adv_shap_values_df.index[:5]}... | Shape: {X_eval_adv_shap_values_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/32\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 12:20:04.257927: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    }
   ],
   "source": [
    "benign_eval_idx = y_eval[y_eval['BENIGN'] == 1].index\n",
    "attack_eval_idx = y_eval[y_eval['ATTACK'] == 1].index\n",
    "\n",
    "pred_benign_idx = y_pred_adv_eval[y_pred_adv_eval['BENIGN'] == 1].index\n",
    "pred_attack_idx = y_pred_adv_eval[y_pred_adv_eval['ATTACK'] == 1].index\n",
    "\n",
    "# predict\n",
    "X_eval_detector = X_eval_adv_shap_values_df.loc[pred_benign_idx]\n",
    "\n",
    "y_pred_eval_detector = det.predict(detector, X_eval_detector, y_eval.columns)\n",
    "\n",
    "correctly_classified_det_idx = y_pred_eval_detector[y_pred_eval_detector['BENIGN'] == 1].index\n",
    "misclassified_det_idx = y_pred_eval_detector[y_pred_eval_detector['ATTACK'] == 1].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='int64')\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step\n",
      "Index([], dtype='int64')\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "detector_misclassifies_attack_as_correct_benign = attack_eval_idx.intersection(correctly_classified_det_idx)\n",
    "print(detector_misclassifies_attack_as_correct_benign)\n",
    "\n",
    "y_pred_test = detector.predict(X_eval_detector)\n",
    "y_pred_test = pd.DataFrame(y_pred_test, index=X_eval_detector.index, columns=y_eval.columns)\n",
    "\n",
    "y_pred_test_index = y_pred_test[y_pred_test['BENIGN'] >= 0.8].index\n",
    "\n",
    "print(detector_misclassifies_attack_as_correct_benign.intersection(y_pred_test_index))\n",
    "print(len(detector_misclassifies_attack_as_correct_benign.intersection(y_pred_test_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDS classifies 'ATTACK' samples as 'ATTACK': 0\n",
      "IDS mis-classifies 'BENIGN' samples as 'ATTACK': 0\n",
      "Detector classifies 'BENIGN' samples as correct 'BENIGN': 43\n",
      "Detector mis-classifies 'ATTACK' samples as correct 'BENIGN': 0\n",
      "Detector classifies 'ATTACK' samples as misclassified due to 'ATTACK': 500\n",
      "Detector classifies 'BENIGN' samples as misclassified due to 'ATTACK': 457\n",
      "TP: 43\n",
      "FP: 0\n",
      "TN: 500\n",
      "FN: 457\n",
      "Sum: 1000\n"
     ]
    }
   ],
   "source": [
    "# After IDS Stage\n",
    "TN = len(attack_eval_idx.intersection(pred_attack_idx)) # IDS classifies 'ATTACK' samples as 'ATTACK'\n",
    "print(f\"IDS classifies 'ATTACK' samples as 'ATTACK': {TN}\")\n",
    "FN = len(benign_eval_idx.intersection(pred_attack_idx)) # IDS classifies 'BENIGN' samples as 'ATTACK'\n",
    "print(f\"IDS mis-classifies 'BENIGN' samples as 'ATTACK': {FN}\")\n",
    "\n",
    "# After Detector Stage\n",
    "TP = len(benign_eval_idx.intersection(correctly_classified_det_idx)) # Detector classifies 'BENIGN' samples as correct 'BENIGN'\n",
    "print(f\"Detector classifies 'BENIGN' samples as correct 'BENIGN': {TP}\")\n",
    "FP = len(attack_eval_idx.intersection(correctly_classified_det_idx)) # Detector classifies 'ATTACK' samples as correct 'BENIGN'\n",
    "print(f\"Detector mis-classifies 'ATTACK' samples as correct 'BENIGN': {FP}\")\n",
    "\n",
    "TN_2 = len(attack_eval_idx.intersection(misclassified_det_idx)) # Detector classifies 'ATTACK' samples as misclassified due to 'ATTACK'\n",
    "print(f\"Detector classifies 'ATTACK' samples as misclassified due to 'ATTACK': {TN_2}\")\n",
    "FN_2 = len(benign_eval_idx.intersection(misclassified_det_idx)) # Detector classifies 'BENIGN' samples as misclassified due to 'ATTACK'\n",
    "print(f\"Detector classifies 'BENIGN' samples as misclassified due to 'ATTACK': {FN_2}\")\n",
    "\n",
    "# Sum up TN & FN from both stages\n",
    "TN = TN + TN_2\n",
    "FN = FN + FN_2\n",
    "\n",
    "print(f\"TP: {TP}\")\n",
    "print(f\"FP: {FP}\")\n",
    "print(f\"TN: {TN}\")\n",
    "print(f\"FN: {FN}\")\n",
    "print(f\"Sum: {TP + FP + TN + FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Accuracy: 96.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ATTACK       0.95      0.98      0.96       500\n",
      "      BENIGN       0.98      0.94      0.96       500\n",
      "\n",
      "    accuracy                           0.96      1000\n",
      "   macro avg       0.96      0.96      0.96      1000\n",
      "weighted avg       0.96      0.96      0.96      1000\n",
      "\n",
      "True Negative Rate: 97.60%\n",
      "False Positive Rate: 2.40%\n",
      "True Positive Rate: 94.40%\n",
      "False Negative Rate: 5.60%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(f\"Global Accuracy: {(TP + TN) / (TP + FP + TN + FN) * 100:.2f}%\")\n",
    "\n",
    "# Construct a fake y_true and y_pred to match sklearn's classification_report format\n",
    "y_true = np.array([1] * TP + [0] * TN + [1] * FN + [0] * FP)  # True labels\n",
    "y_pred = np.array([1] * TP + [0] * TN + [0] * FN + [1] * FP)  # Predicted labels\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_true, y_pred, target_names=['ATTACK', 'BENIGN']) # reverse labels because classification_report assumes first label is 0\n",
    "print(report)\n",
    "\n",
    "print(f\"True Negative Rate: {TN/(TN+FP)*100:.2f}%\")\n",
    "print(f\"False Positive Rate: {FP/(TN+FP)*100:.2f}%\")\n",
    "print(f\"True Positive Rate: {TP/(TP+FN)*100:.2f}%\")\n",
    "print(f\"False Negative Rate: {FN/(TP+FN)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
