{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype: iota\n",
    "\n",
    "| Properties      | Data    |\n",
    "|---------------|-----------|\n",
    "| *Labels* | `['BENIGN', 'DDoS']` |\n",
    "| *Normalization* | `Min-Max` |\n",
    "| *Sample Size* | `10.000`|\n",
    "| *Adversarial Attack* | `FGSM & C&W` |\n",
    "| *Explanations* | `SHAP` |\n",
    "| *Detector* | `Detect misclassified Samples of both Attacks` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Has to be run first alone!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To import modules from the functions directory\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Building CICIDS2017 dataset --\n",
      "--- Combining all CICIDS2017 files ---\n",
      "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "Wednesday-workingHours.pcap_ISCX.csv\n",
      "Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "Monday-WorkingHours.pcap_ISCX.csv\n",
      "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "--- Removing NaN and Infinity values ---\n",
      "Removing 1358 Rows with NaN values\n",
      "Removing 1509 Rows with Infinity values\n",
      "--- Extracting labels ---\n",
      " Label\n",
      "BENIGN    2271320\n",
      "DDoS       128025\n",
      "Name: count, dtype: int64\n",
      "-- Generating normalizer --\n",
      "--- Splitting labels and features ---\n",
      "Zero Columns: [' Bwd PSH Flags', ' Bwd URG Flags', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate']\n",
      "-- Preprocessing data --\n",
      "--- Sampling balanced data ---\n",
      "Sample to shape: (10000, 79)\n",
      "--- Splitting labels and features ---\n",
      "--- Encoding labels as binary one-hot values ---\n",
      "--- Normalizing features using MinMaxScaler ---\n",
      "Generate Features | Indices: Index([1787989, 978677, 500719, 1942857, 2150382], dtype='int64')... | Shape: (10000, 70)\n",
      "Generate Labels | Indices: Index([1787989, 978677, 500719, 1942857, 2150382], dtype='int64')... | Shape: (10000, 2)\n",
      "BENIGN  ATTACK\n",
      "False   True      5000\n",
      "True    False     5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import functions.data_preprocessing as dp\n",
    "import importlib\n",
    "importlib.reload(dp)\n",
    "\n",
    "encoding_type = 0 # binary encoding\n",
    "norm_type = 0 # min-max normalization\n",
    "label_names = ['BENIGN', 'DDoS'] # labels to include\n",
    "sample_size = 5000 # sample size for each label -> 2 x sample_size = total samples\n",
    "\n",
    "dataset = dp.build_dataset(label_names)\n",
    "\n",
    "normalizer, zero_columns = dp.generate_normalizer(dataset, norm_type)\n",
    "\n",
    "feature_df, label_df, used_indices = dp.preprocess_data(dataset, encoding_type, normalizer, zero_columns, sample_size=sample_size, random_sample_state=42)\n",
    "print(f\"Generate Features | Indices: {feature_df.index[:5]}... | Shape: {feature_df.shape}\")\n",
    "print(f\"Generate Labels | Indices: {label_df.index[:5]}... | Shape: {label_df.shape}\")\n",
    "print(label_df.value_counts()) # -> will first show [0, 1] then [1, 0] if label number is equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 70) (2000, 70) (8000, 2) (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, label_df, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8144 - loss: 0.5964 - val_accuracy: 0.9775 - val_loss: 0.2770\n",
      "Epoch 2/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9747 - loss: 0.1938 - val_accuracy: 0.9812 - val_loss: 0.0661\n",
      "Epoch 3/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9803 - loss: 0.0637 - val_accuracy: 0.9837 - val_loss: 0.0508\n",
      "Epoch 4/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9827 - loss: 0.0506 - val_accuracy: 0.9856 - val_loss: 0.0433\n",
      "Epoch 5/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9834 - loss: 0.0445 - val_accuracy: 0.9856 - val_loss: 0.0378\n",
      "Epoch 6/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9835 - loss: 0.0412 - val_accuracy: 0.9862 - val_loss: 0.0341\n",
      "Epoch 7/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9843 - loss: 0.0384 - val_accuracy: 0.9881 - val_loss: 0.0323\n",
      "Epoch 8/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9847 - loss: 0.0367 - val_accuracy: 0.9887 - val_loss: 0.0306\n",
      "Epoch 9/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9856 - loss: 0.0352 - val_accuracy: 0.9887 - val_loss: 0.0293\n",
      "Epoch 10/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9862 - loss: 0.0340 - val_accuracy: 0.9894 - val_loss: 0.0279\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step\n",
      "Global Accuracy: 98.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ATTACK       0.96      1.00      0.98       988\n",
      "      BENIGN       1.00      0.96      0.98      1012\n",
      "\n",
      "    accuracy                           0.98      2000\n",
      "   macro avg       0.98      0.98      0.98      2000\n",
      "weighted avg       0.98      0.98      0.98      2000\n",
      "\n",
      "\u001b[1m163/250\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 310us/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 14:21:59.782289: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step\n",
      "Predictions on Normal Data | Indices: Index([383258, 1283627, 1595713, 323550, 474229], dtype='int64')... | Shape: (8000, 2)\n"
     ]
    }
   ],
   "source": [
    "import functions.intrusion_detection_system as ids\n",
    "import importlib\n",
    "importlib.reload(ids)\n",
    "\n",
    "# TODO: build ids with complete dataset\n",
    "# X_train_all, y_train_all, _ = dp.preprocess_data(dataset, encoding_type, normalizer, zero_columns, random_sample_state=42)\n",
    "# print(y_train_all.value_counts())\n",
    "# X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=42)\n",
    "# print(X_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape)\n",
    "\n",
    "# build ids and evaluate it on test data\n",
    "ids_model = ids.build_intrusion_detection_system(X_train, y_train, X_test, y_test)\n",
    "# store prediction from X_train\n",
    "y_pred = ids.predict(ids_model, X_train, columns=y_train.columns)\n",
    "print(f\"Predictions on Normal Data | Indices: {y_pred.index[:5]}... | Shape: {y_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate C&W and FGSM Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 70) (4000, 70) (4000, 2) (4000, 2)\n",
      "Adversarial FGSM examples generated. Shape: (4000, 70)\n",
      "Create Adversarial Attack | Indices: Index([305763, 2410673, 473894, 311088, 1632424], dtype='int64')... | Shape: (4000, 70)\n",
      "Accuracy: 49.70%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ATTACK       0.00      0.00      0.00      2012\n",
      "      BENIGN       0.50      1.00      0.66      1988\n",
      "\n",
      "    accuracy                           0.50      4000\n",
      "   macro avg       0.25      0.50      0.33      4000\n",
      "weighted avg       0.25      0.50      0.33      4000\n",
      "\n",
      "Confusion Matrix: Positive == BENIGN\n",
      "TN: 0, FP: 2012, FN: 0, TP: 1988\n",
      "Predictions on Adversarial Attacks | Indices: Index([305763, 2410673, 473894, 311088, 1632424], dtype='int64')... | Shape: (4000, 2)\n",
      "Running attack using 24 CPU cores...\n",
      "\n",
      "Process 132176 is generating adversarial examples for batch of size 166 \n",
      "Process 132177 is generating adversarial examples for batch of size 166 \n",
      "Process 132175 is generating adversarial examples for batch of size 166 \n",
      "Process 132178 is generating adversarial examples for batch of size 166 \n",
      "Process 132179 is generating adversarial examples for batch of size 166 \n",
      "Process 132180 is generating adversarial examples for batch of size 166 \n",
      "Process 132182 is generating adversarial examples for batch of size 166 \n",
      "\n",
      "Process 132181 is generating adversarial examples for batch of size 166 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Process 132183 is generating adversarial examples for batch of size 166 \n",
      "\n",
      "Process 132184 is generating adversarial examples for batch of size 166 \n",
      "Process 132186 is generating adversarial examples for batch of size 166 \n",
      "Process 132185 is generating adversarial examples for batch of size 166 \n",
      "Process 132187 is generating adversarial examples for batch of size 166 \n",
      "\n",
      "\n",
      "Process 132188 is generating adversarial examples for batch of size 166 \n",
      "Process 132189 is generating adversarial examples for batch of size 166 \n",
      "\n",
      "\n",
      "Process 132190 is generating adversarial examples for batch of size 166 \n",
      "Process 132191 is generating adversarial examples for batch of size 166 \n",
      "\n",
      "\n",
      "\n",
      "Process 132192 is generating adversarial examples for batch of size 166 \n",
      "Process 132193 is generating adversarial examples for batch of size 166 \n",
      "\n",
      "\n",
      "Process 132194 is generating adversarial examples for batch of size 166 \n",
      "Process 132195 is generating adversarial examples for batch of size 166 \n",
      "Process 132196 is generating adversarial examples for batch of size 166 \n",
      "\n",
      "\n",
      "Process 132197 is generating adversarial examples for batch of size 166 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Process 132198 is generating adversarial examples for batch of size 182 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660db6883f834be282a38af73733975d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13df218fdf684e7bad1707a9f52b2697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b541e69c7fbe4282870e5c2432c95f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e55ad4e9ebe483aa93efe7f9266f7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e4e59ac1e34e3c814d4926e706a254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7ee5a79cb447218a567b110d16824a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4ec4a963f349418ff1f922b4a13680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160e7b96188b4ad98834eed212b7c2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67bf2b1a8584852a5001993ae0d5a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46248195c4a4cd7bd9b7db2bcdd7b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ee5eb17b4c4c22b33544d20f759cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c913011f4c344235bc4e70b707997d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ade4f56fc34113b322809421e448c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2efc9f6b703481bb56b486984f8942a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b56b3cdf0bf4327bc0b04c989cb4ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f691821729499e8646f15c176c716a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eaac655a78d4cadac617ae9b933df27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300e5e3bb2854acf864fb258611d7c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f663438014c94794b119be97fa1ec4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1c4eb2de5a475ebc79b1604c4fd05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a597fc1218f4d83967ee904bbcfd94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027dd0bc3cbe4a808c8cbdb3dad593f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29ae2f6d8e54f6b99b823e8d2332686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec03284a6c6a442a999bb710f2e44864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Adversarial Attack | Indices: Index([358687, 2773523, 323364, 430929, 467866], dtype='int64')... | Shape: (4000, 70)\n",
      "Accuracy: 75.55%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ATTACK       1.00      0.51      0.68      2000\n",
      "      BENIGN       0.67      1.00      0.80      2000\n",
      "\n",
      "    accuracy                           0.76      4000\n",
      "   macro avg       0.84      0.76      0.74      4000\n",
      "weighted avg       0.84      0.76      0.74      4000\n",
      "\n",
      "Confusion Matrix: Positive == BENIGN\n",
      "TN: 1022, FP: 978, FN: 0, TP: 2000\n",
      "Predictions on Adversarial Attacks | Indices: Index([358687, 2773523, 323364, 430929, 467866], dtype='int64')... | Shape: (4000, 2)\n"
     ]
    }
   ],
   "source": [
    "import functions.attack_generator as ag\n",
    "import importlib\n",
    "import numpy as np\n",
    "importlib.reload(ag)\n",
    "\n",
    "all_features = dataset.drop(columns=[' Label'])\n",
    "art_model = ag.convert_to_art_model(ids_model, X_train) # TODO: use all features for generating art model\n",
    "\n",
    "# split train data into data for generating fgsm and cw attacks\n",
    "X_fgsm, X_cw, y_fgsm, y_cw = train_test_split(X_train, y_train, test_size=0.5, random_state=15)\n",
    "print(X_fgsm.shape, X_cw.shape, y_fgsm.shape, y_cw.shape)\n",
    "\n",
    "# generate attacks on the separated training data\n",
    "# TODO: when changing epsilon, the detector accuracy rises\n",
    "X_adv_fgsm = ag.generate_fgsm_attacks(art_model, X_fgsm, 1)\n",
    "print(f\"Create Adversarial Attack | Indices: {X_adv_fgsm.index[:5]}... | Shape: {X_adv_fgsm.shape}\")\n",
    "y_pred_adv_fgsm = ag.evaluate_art_model(art_model, X_adv_fgsm, y_fgsm)\n",
    "print(f\"Predictions on Adversarial Attacks | Indices: {y_pred_adv_fgsm.index[:5]}... | Shape: {y_pred_adv_fgsm.shape}\")\n",
    "y_pred_fgsm = y_pred.loc[X_fgsm.index]\n",
    "\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "X_adv_cw = ag.generate_cw_attacks_parallel(art_model, X_cw, 1, num_cores=num_cores)\n",
    "print(f\"Create Adversarial Attack | Indices: {X_adv_cw.index[:5]}... | Shape: {X_adv_cw.shape}\")\n",
    "y_pred_adv_cw = ag.evaluate_art_model(art_model, X_adv_cw, y_cw)\n",
    "print(f\"Predictions on Adversarial Attacks | Indices: {y_pred_adv_cw.index[:5]}... | Shape: {y_pred_adv_cw.shape}\")\n",
    "y_pred_cw = y_pred.loc[X_cw.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correctly_benign_classified_indices(y_train, y_pred):\n",
    "    benign_indices = y_train[y_train['BENIGN'] == 1].index\n",
    "    benign_adv_predicted_indices = y_pred[y_pred['BENIGN'] == 1].index\n",
    "    correctly_benign_classified_indices = benign_indices.intersection(benign_adv_predicted_indices)\n",
    "    return correctly_benign_classified_indices\n",
    "\n",
    "def get_misclassified_as_benign_due_attack_indices(y_train, y_pred, y_pred_adv):\n",
    "    attack_indices = y_train[y_train['ATTACK'] == 1].index\n",
    "    attack_adv_predicted_indices = y_pred[y_pred['ATTACK'] == 1].index\n",
    "    benign_predicted_adversarial_indices = y_pred_adv[y_pred_adv['BENIGN'] == 1].index\n",
    "    misclassified_as_benign_due_attack_indices = attack_indices.intersection(attack_adv_predicted_indices).intersection(benign_predicted_adversarial_indices)\n",
    "    return misclassified_as_benign_due_attack_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly classified as BENIGN from the IDS: 3878 | Indices: Index([1283627, 1595713, 953730], dtype='int64')\n",
      "Correctly classified as BENIGN from the IDS (FGSM): 1942 | Indices: Index([2410673, 1632424, 238489], dtype='int64')\n",
      "ATTACK sample misclassified as BENIGN due to adversarial attack (FGSM): 2009 | Indices: Index([305763, 473894, 311088], dtype='int64')\n",
      "Correctly classified as BENIGN from the IDS (CW): 1936 | Indices: Index([2773523, 2550577, 1631321], dtype='int64')\n",
      "ATTACK sample misclassified as BENIGN due to adversarial attack (CW): 976 | Indices: Index([323364, 430929, 467866], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "correctly_benign_classified_indices = get_correctly_benign_classified_indices(y_train, y_pred)\n",
    "print(f\"Correctly classified as BENIGN from the IDS: {len(correctly_benign_classified_indices)} | Indices: {correctly_benign_classified_indices[:3]}\")\n",
    "\n",
    "correctly_benign_classified_indices_fgsm = get_correctly_benign_classified_indices(y_fgsm, y_pred_fgsm)\n",
    "misclassified_as_benign_due_attack_indices_fgsm = get_misclassified_as_benign_due_attack_indices(y_fgsm, y_pred_fgsm, y_pred_adv_fgsm)\n",
    "print(f\"Correctly classified as BENIGN from the IDS (FGSM): {len(correctly_benign_classified_indices_fgsm)} | Indices: {correctly_benign_classified_indices_fgsm[:3]}\")\n",
    "print(f\"ATTACK sample misclassified as BENIGN due to adversarial attack (FGSM): {len(misclassified_as_benign_due_attack_indices_fgsm)} | Indices: {misclassified_as_benign_due_attack_indices_fgsm[:3]}\")\n",
    "\n",
    "correctly_benign_classified_indices_cw = get_correctly_benign_classified_indices(y_cw, y_pred_cw)\n",
    "misclassified_as_benign_due_attack_indices_cw = get_misclassified_as_benign_due_attack_indices(y_cw, y_pred_cw, y_pred_adv_cw)\n",
    "print(f\"Correctly classified as BENIGN from the IDS (CW): {len(correctly_benign_classified_indices_cw)} | Indices: {correctly_benign_classified_indices_cw[:3]}\")\n",
    "print(f\"ATTACK sample misclassified as BENIGN due to adversarial attack (CW): {len(misclassified_as_benign_due_attack_indices_cw)} | Indices: {misclassified_as_benign_due_attack_indices_cw[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions.visualizer as visualizer\n",
    "# import importlib\n",
    "# importlib.reload(visualizer)\n",
    "\n",
    "# visualizer.visualize_data_distribution(X_train.loc[correctly_benign_classified_indices], 'Normal Data', X_adv_fgsm.loc[misclassified_as_benign_due_attack_indices_fgsm], 'Adversarial Data')\n",
    "# # visualizer.pca_visualization_side_by_side(X_train.loc[misclassified_as_benign_due_attack_indices], 'Normal Data', X_adv.loc[misclassified_as_benign_due_attack_indices], 'Adversarial Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 8001it [04:04, 31.39it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Explanations | Indices: Index([383258, 1283627, 1595713, 323550, 474229], dtype='int64')... | Shape: (8000, 70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 4001it [01:54, 31.88it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Adversarial Explanations | Indices: Index([305763, 2410673, 473894, 311088, 1632424], dtype='int64')... | Shape: (4000, 70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 4001it [02:06, 29.23it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Adversarial Explanations | Indices: Index([358687, 2773523, 323364, 430929, 467866], dtype='int64')... | Shape: (4000, 70)\n"
     ]
    }
   ],
   "source": [
    "import functions.explainer as exp\n",
    "import importlib\n",
    "importlib.reload(exp)\n",
    "\n",
    "explainer = exp.generate_shap_explainer(ids_model, X_train)\n",
    "\n",
    "shap_values, shap_values_df = exp.generate_shap_values(explainer, X_train)\n",
    "print(f\"Generate Explanations | Indices: {shap_values_df.index[:5]}... | Shape: {shap_values_df.shape}\")\n",
    "\n",
    "_, shap_values_adv_df_fgsm = exp.generate_shap_values(explainer, X_adv_fgsm)\n",
    "print(f\"Generate Adversarial Explanations | Indices: {shap_values_adv_df_fgsm.index[:5]}... | Shape: {shap_values_adv_df_fgsm.shape}\")\n",
    "\n",
    "_, shap_values_adv_df_cw = exp.generate_shap_values(explainer, X_adv_cw)\n",
    "print(f\"Generate Adversarial Explanations | Indices: {shap_values_adv_df_cw.index[:5]}... | Shape: {shap_values_adv_df_cw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # concat_correctly_benign_classified_shaps = pd.concat([shap_values_df.loc[correctly_benign_classified_indices], shap_values_adv_df_fgsm.loc[correctly_benign_classified_indices_fgsm], shap_values_adv_df_cw.loc[correctly_benign_classified_indices_cw]], axis=0)\n",
    "# # # shap_values_df.loc[misclassified_as_benign_due_attack_indices]\n",
    "# # concat_misclassified_as_benign_shaps = pd.concat([shap_values_adv_df_fgsm.loc[misclassified_as_benign_due_attack_indices_fgsm], shap_values_adv_df_cw.loc[misclassified_as_benign_due_attack_indices_cw]], axis=0)\n",
    "\n",
    "# concat_correctly_benign_classified_shaps = pd.concat([shap_values_df.loc[correctly_benign_classified_indices], shap_values_adv_df_fgsm.loc[correctly_benign_classified_indices_fgsm]], axis=0)\n",
    "# # shap_values_df.loc[misclassified_as_benign_due_attack_indices]\n",
    "# concat_misclassified_as_benign_shaps = pd.concat([shap_values_adv_df_fgsm.loc[misclassified_as_benign_due_attack_indices_fgsm]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions.visualizer as visualizer\n",
    "# import importlib\n",
    "# importlib.reload(visualizer)\n",
    "\n",
    "# visualizer.visualize_data_distribution(shap_values_df.loc[correctly_benign_classified_indices], 'Normal Explanations', shap_values_adv_df_fgsm.loc[misclassified_as_benign_due_attack_indices_fgsm], 'Adversarial Explanations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp.plot_shap_summary_comparison(shap_values_df.loc[correctly_benign_classified_indices].values, X_train.loc[correctly_benign_classified_indices], shap_values_adv_df.loc[misclassified_as_benign_due_attack_indices].values, X_adv.loc[misclassified_as_benign_due_attack_indices], 6, title='Normal vs Adversarial Explanations of Benign Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Correctly Classified as Benign: 3878\n",
      "Normal Misclassified as Benign: 5\n",
      "Adversarial Correctly Classified as Benign (CW): 1936\n",
      "Adversarial Misclassified as Benign (CW): 976\n",
      "Adversarial Correctly Classified as Benign (FGSM): 1942\n",
      "Adversarial Misclassified as Benign (FGSM): 2009\n"
     ]
    }
   ],
   "source": [
    "adversarial_correct_benign_indices_cw = correctly_benign_classified_indices_cw\n",
    "adversarial_misclassified_as_benign_indices_cw = misclassified_as_benign_due_attack_indices_cw\n",
    "adversarial_correct_benign_indices_fgsm = correctly_benign_classified_indices_fgsm\n",
    "adversarial_misclassified_as_benign_indices_fgsm = misclassified_as_benign_due_attack_indices_fgsm\n",
    "normal_correct_benign_indices = correctly_benign_classified_indices\n",
    "\n",
    "attack_indices = y_train[y_train['ATTACK'] == 1].index\n",
    "predicted_benign_indices = y_pred[y_pred['BENIGN'] == 1].index\n",
    "normal_misclassified_as_benign_indices = attack_indices.intersection(predicted_benign_indices)\n",
    "\n",
    "print(f\"Normal Correctly Classified as Benign: {len(normal_correct_benign_indices)}\")\n",
    "print(f\"Normal Misclassified as Benign: {len(normal_misclassified_as_benign_indices)}\")\n",
    "print(f\"Adversarial Correctly Classified as Benign (CW): {len(adversarial_correct_benign_indices_cw)}\")\n",
    "print(f\"Adversarial Misclassified as Benign (CW): {len(adversarial_misclassified_as_benign_indices_cw)}\")\n",
    "print(f\"Adversarial Correctly Classified as Benign (FGSM): {len(adversarial_correct_benign_indices_fgsm)}\")\n",
    "print(f\"Adversarial Misclassified as Benign (FGSM): {len(adversarial_misclassified_as_benign_indices_fgsm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# shap_adv_correct_benign = shap_values_adv_df_cw.loc[adversarial_correct_benign_indices_cw]\n",
    "# shap_adv_misclassified_benign = shap_values_adv_df_cw.loc[adversarial_misclassified_as_benign_indices_cw]\n",
    "# shap_normal_correct_benign = shap_values_df.loc[normal_correct_benign_indices]\n",
    "# shap_normal_misclassified_benign = shap_values_df.loc[normal_misclassified_as_benign_indices]\n",
    "# print(shap_adv_correct_benign.shape, shap_adv_misclassified_benign.shape, shap_normal_correct_benign.shape, shap_normal_misclassified_benign.shape)\n",
    "# print('ADV CORRECT BENIGN |', 'ADV MISCLASSIFIED BENIGN |', 'NORM CORRECT BENIGN |', 'NORM MISCLASSIFIED BENIGN')\n",
    "\n",
    "\n",
    "# # build dataset\n",
    "# y_adv_benign = np.array([[1, 0, 0, 0]] * shap_adv_correct_benign.shape[0])  \n",
    "# y_adv_attack = np.array([[0, 1, 0, 0]] * shap_adv_misclassified_benign.shape[0])\n",
    "# y_norm_bening = np.array([[0, 0, 1, 0]] * shap_normal_correct_benign.shape[0])\n",
    "# y_norm_attack = np.array([[0, 0, 0, 1]] * shap_normal_misclassified_benign.shape[0])\n",
    "\n",
    "\n",
    "# y = np.concatenate([y_adv_benign, y_adv_attack, y_norm_bening, y_norm_attack])\n",
    "# y = pd.DataFrame(y, columns=['ADV CORRECT BENIGN', 'ADV MISCLASSIFIED BENIGN', 'NORM CORRECT BENIGN', 'NORM MISCLASSIFIED BENIGN'])\n",
    "\n",
    "# X = pd.concat([shap_adv_correct_benign, shap_adv_misclassified_benign, shap_normal_correct_benign, shap_normal_misclassified_benign], axis=0)\n",
    "# print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_dataset(class_samples):\n",
    "    \"\"\"\n",
    "    Create dataset from given class samples.\n",
    "    \n",
    "    Args:\n",
    "        class_samples (dict): Dictionary where keys are class names and values are DataFrames of samples.\n",
    "    \n",
    "    Returns:\n",
    "        X (pd.DataFrame): Feature matrix\n",
    "        y (pd.DataFrame): One-hot encoded labels\n",
    "    \"\"\"\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    class_labels = list(class_samples.keys())\n",
    "    num_classes = len(class_labels)\n",
    "    \n",
    "    for i, class_name in enumerate(class_labels):\n",
    "        samples = class_samples[class_name]\n",
    "        one_hot = np.zeros((samples.shape[0], num_classes))\n",
    "        one_hot[:, i] = 1  # Set the respective column to 1\n",
    "        \n",
    "        X_list.append(samples)\n",
    "        y_list.append(one_hot)\n",
    "    \n",
    "    X = pd.concat(X_list, axis=0)\n",
    "    y = np.vstack(y_list)\n",
    "    y = pd.DataFrame(y, columns=class_labels)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8854, 70) (8854, 3)\n"
     ]
    }
   ],
   "source": [
    "class_samples = {\n",
    "    # 'ADV CW BENIGN': shap_values_adv_df_cw.loc[adversarial_correct_benign_indices_cw],\n",
    "    'ADV CW ATTACK MISCLASSIFIED': shap_values_adv_df_cw.loc[adversarial_misclassified_as_benign_indices_cw],\n",
    "    # 'ADV FGSM BENIGN': shap_values_adv_df_fgsm.loc[adversarial_correct_benign_indices_fgsm],\n",
    "    'ADV FGSM': shap_values_adv_df_fgsm,\n",
    "    'NORM BENIGN': shap_values_df.loc[normal_correct_benign_indices]\n",
    "}\n",
    "\n",
    "X, y = create_dataset(class_samples)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8854, 70) (8854, 3)\n",
      "(7968, 70) (886, 70) (7968, 3) (886, 3)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 14:41:25.408358: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4403 - loss: 0.6753 - val_accuracy: 0.4987 - val_loss: 0.5868\n",
      "Epoch 2/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6011 - loss: 0.5457 - val_accuracy: 0.8896 - val_loss: 0.3263\n",
      "Epoch 3/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8327 - loss: 0.3346 - val_accuracy: 0.9605 - val_loss: 0.1579\n",
      "Epoch 4/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9167 - loss: 0.2188 - val_accuracy: 0.9737 - val_loss: 0.0972\n",
      "Epoch 5/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9380 - loss: 0.1522 - val_accuracy: 0.9843 - val_loss: 0.0592\n",
      "Epoch 6/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9448 - loss: 0.1094 - val_accuracy: 0.9893 - val_loss: 0.0374\n",
      "Epoch 7/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9525 - loss: 0.0802 - val_accuracy: 0.9906 - val_loss: 0.0272\n",
      "Epoch 8/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9553 - loss: 0.0673 - val_accuracy: 0.9918 - val_loss: 0.0216\n",
      "Epoch 9/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9668 - loss: 0.0537 - val_accuracy: 0.9918 - val_loss: 0.0176\n",
      "Epoch 10/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9725 - loss: 0.0494 - val_accuracy: 0.9931 - val_loss: 0.0153\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "import functions.detector as det\n",
    "import importlib\n",
    "importlib.reload(det)\n",
    "\n",
    "# build detector to detect adversarial samples that misclassify attack samples as benign\n",
    "\n",
    "# create dataframe\n",
    "# TODO: build detector with normal and adversarial shap values?\n",
    "# TODO: build with shap_values_adv_df to detect 'BENIGN' and 'ATTACK'\n",
    "import pandas as pd\n",
    "\n",
    "# alternative approach: detector that predicts the original label of the sample for all given adversarial attacks\n",
    "# concat_correctly_benign_classified_shaps = pd.concat([shap_values_df.loc[correctly_benign_classified_indices], shap_values_adv_df.loc[correctly_benign_classified_indices]], axis=0)\n",
    "# concat_misclassified_as_benign_shaps = pd.concat([shap_values_df.loc[misclassified_as_benign_due_attack_indices], shap_values_adv_df.loc[misclassified_as_benign_due_attack_indices]], axis=0)\n",
    "# X, y = det.build_train_datasets(shap_values_df.loc[correctly_benign_classified_indices], shap_values_adv_df_fgsm.loc[misclassified_as_benign_due_attack_indices_fgsm])\n",
    "\n",
    "#X, y = det.build_train_datasets(shap_values_df.loc[correctly_benign_classified_indices], shap_values_adv_df.loc[misclassified_as_benign_due_attack_indices])\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# split data\n",
    "X_train_det, X_test_det, y_train_det, y_test_det = train_test_split(X, y, test_size=0.1, random_state=1503)\n",
    "print(X_train_det.shape, X_test_det.shape, y_train_det.shape, y_test_det.shape)\n",
    "\n",
    "# build detector\n",
    "detector = det.build_detector(X_train_det, y_train_det, X_test_det, y_test_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Predictions on Detector | Indices: Index([422234, 272630, 435345, 379069, 311517], dtype='int64')... | Shape: (886, 3)\n",
      "[1 1 1 1 1] [1 1 1 1 1]\n",
      "Overall Accuracy: 0.9910\n",
      "Classification Report (Overall):\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "ADV CW ATTACK MISCLASSIFIED       0.97      0.97      0.97       120\n",
      "                   ADV FGSM       1.00      1.00      1.00       407\n",
      "                NORM BENIGN       0.99      0.99      0.99       359\n",
      "\n",
      "                   accuracy                           0.99       886\n",
      "                  macro avg       0.99      0.99      0.99       886\n",
      "               weighted avg       0.99      0.99      0.99       886\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 16:17:32.628114: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate detector\n",
    "y_pred_det = det.predict(detector, X_test_det, y.columns)\n",
    "print(f\"Predictions on Detector | Indices: {y_pred_det.index[:5]}... | Shape: {y_pred_det.shape}\")\n",
    "\n",
    "# Convert one-hot to class indices\n",
    "y_true_indices = np.argmax(y_test_det, axis=1)\n",
    "y_pred_indices = np.argmax(y_pred_det, axis=1)\n",
    "print(y_true_indices[:5], y_pred_indices[:5])\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Compute Accuracy\n",
    "accuracy = accuracy_score(y_true_indices, y_pred_indices)\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compute Classification Report for overall classification\n",
    "print(\"Classification Report (Overall):\")\n",
    "print(classification_report(y_true_indices, y_pred_indices, target_names=y.columns, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976\n",
      "2009\n"
     ]
    }
   ],
   "source": [
    "# find misclassified samples\n",
    "print(len(adversarial_misclassified_as_benign_indices_cw))\n",
    "print(len(adversarial_misclassified_as_benign_indices_fgsm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1] [1 1 1 1 1]\n",
      "[1 1 1 0 1] [1 1 1 0 1]\n",
      "\n",
      "Adversarial Detection (Class 1 + 2):\n",
      "TP (Adversarial correctly detected): 227\n",
      "FP (Benign incorrectly detected as adversarial): 164\n",
      "TN (Benign correctly detected): 0\n",
      "FN (Adversarial missed): 0\n",
      "Adversarial Detection Metrics:\n",
      "Adversarial Detection Accuracy: 58.06%\n",
      "True Positive Rate (TPR): 100.00%\n",
      "False Positive Rate (FPR): 100.00%\n",
      "False Negative Rate (FNR): 0.00%\n",
      "True Negative Rate (TNR): 0.00%\n",
      "\n",
      "Benign Detection (Class 1 + 3):\n",
      "TP (Benign correctly detected): 305\n",
      "FP (Adversarial incorrectly detected as benign): 0\n",
      "TN (Adversarial correctly detected): 84\n",
      "FN (Benign missed): 2\n",
      "Benign Detection Metrics:\n",
      "Benign Detection Accuracy: 99.49%\n",
      "True Positive Rate (TPR): 99.35%\n",
      "False Positive Rate (FPR): 0.00%\n",
      "False Negative Rate (FNR): 0.65%\n",
      "True Negative Rate (TNR): 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Combine classes for adversarial detection (class 1 and class 2)\n",
    "y_true_adv = np.where(np.isin(y_true_indices, [0, 1]), 1, 0)  # Adversarial = 1 (class 1 or 2), otherwise 0\n",
    "y_pred_adv = np.where(np.isin(y_pred_indices, [0, 1]), 1, 0)  # Predicted as Adversarial\n",
    "print(y_true_adv[:5], y_pred_adv[:5])\n",
    "\n",
    "# Combine classes for benign detection (class 1 and class 3)\n",
    "y_true_benign = np.where(np.isin(y_true_indices, [0, 2]), 1, 0)  # Benign = 1 (class 1 or 3), otherwise 0\n",
    "y_pred_benign = np.where(np.isin(y_pred_indices, [0, 2]), 1, 0)  # Predicted as Benign\n",
    "print(y_true_benign[:5], y_pred_benign[:5])\n",
    "\n",
    "\n",
    "# Compute confusion matrix for Adversarial Detection\n",
    "tn_adv, fp_adv, fn_adv, tp_adv = confusion_matrix(y_true_adv, y_pred_adv).ravel()\n",
    "print(f\"\\nAdversarial Detection (Class 1 + 2):\")\n",
    "print(f\"TP (Adversarial correctly detected): {tp_adv}\")\n",
    "print(f\"FP (Benign incorrectly detected as adversarial): {fp_adv}\")\n",
    "print(f\"TN (Benign correctly detected): {tn_adv}\")\n",
    "print(f\"FN (Adversarial missed): {fn_adv}\")\n",
    "\n",
    "# Calculate metrics for Adversarial Detection\n",
    "tpr_adv = tp_adv / (tp_adv + fn_adv) if (tp_adv + fn_adv) != 0 else 0\n",
    "fpr_adv = fp_adv / (fp_adv + tn_adv) if (fp_adv + tn_adv) != 0 else 0\n",
    "fnr_adv = fn_adv / (tp_adv + fn_adv) if (tp_adv + fn_adv) != 0 else 0\n",
    "tnr_adv = tn_adv / (tn_adv + fp_adv) if (tn_adv + fp_adv) != 0 else 0  \n",
    "\n",
    "# Calculate accuracy for Adversarial Detection\n",
    "accuracy_adv = (tp_adv + tn_adv) / (tp_adv + tn_adv + fp_adv + fn_adv) if (tp_adv + tn_adv + fp_adv + fn_adv) != 0 else 0\n",
    "\n",
    "print(f\"Adversarial Detection Metrics:\")\n",
    "print(f\"Adversarial Detection Accuracy: {100*accuracy_adv:.2f}%\")\n",
    "print(f\"True Positive Rate (TPR): {100*tpr_adv:.2f}%\")\n",
    "print(f\"False Positive Rate (FPR): {100*fpr_adv:.2f}%\")\n",
    "print(f\"False Negative Rate (FNR): {100* fnr_adv:.2f}%\")\n",
    "print(f\"True Negative Rate (TNR): {100*tnr_adv:.2f}%\") \n",
    "\n",
    "# Compute confusion matrix for Benign Detection\n",
    "tn_benign, fp_benign, fn_benign, tp_benign = confusion_matrix(y_true_benign, y_pred_benign).ravel()\n",
    "print(f\"\\nBenign Detection (Class 1 + 3):\")\n",
    "print(f\"TP (Benign correctly detected): {tp_benign}\")\n",
    "print(f\"FP (Adversarial incorrectly detected as benign): {fp_benign}\")\n",
    "print(f\"TN (Adversarial correctly detected): {tn_benign}\")\n",
    "print(f\"FN (Benign missed): {fn_benign}\")\n",
    "\n",
    "# Calculate metrics for Benign Detection\n",
    "tpr_benign = tp_benign / (tp_benign + fn_benign) if (tp_benign + fn_benign) != 0 else 0\n",
    "fpr_benign = fp_benign / (fp_benign + tn_benign) if (fp_benign + tn_benign) != 0 else 0\n",
    "fnr_benign = fn_benign / (tp_benign + fn_benign) if (tp_benign + fn_benign) != 0 else 0\n",
    "tnr_benign = tn_benign / (tn_benign + fp_benign) if (tn_benign + fp_benign) != 0 else 0  \n",
    "\n",
    "# Calculate accuracy for Benign Detection\n",
    "accuracy_benign = (tp_benign + tn_benign) / (tp_benign + tn_benign + fp_benign + fn_benign) if (tp_benign + tn_benign + fp_benign + fn_benign) != 0 else 0\n",
    "\n",
    "print(f\"Benign Detection Metrics:\")\n",
    "print(f\"Benign Detection Accuracy: {100*accuracy_benign:.2f}%\")\n",
    "print(f\"True Positive Rate (TPR): {100*tpr_benign:.2f}%\")\n",
    "print(f\"False Positive Rate (FPR): {100*fpr_benign:.2f}%\")\n",
    "print(f\"False Negative Rate (FNR): {100*fnr_benign:.2f}%\")\n",
    "print(f\"True Negative Rate (TNR): {100*tnr_benign:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Samples: 0 | []\n",
      "Benign Samples: 305 | [0 1 2 4 6]\n",
      "Intersection of Adversarial and Benign Samples: 0\n",
      "Normal Samples: 0 | []\n"
     ]
    }
   ],
   "source": [
    "# find intersection of benign and normal samples\n",
    "normal_samples = np.where(y_pred_adv == 0)[0]\n",
    "print(f\"Normal Samples: {len(normal_samples)} | {normal_samples[:5]}\")\n",
    "normal_samples = set(normal_samples)\n",
    "benign_samples = np.where(y_pred_benign == 1)[0]\n",
    "print(f\"Benign Samples: {len(benign_samples)} | {benign_samples[:5]}\")\n",
    "benign_samples = set(benign_samples)\n",
    "intersection = normal_samples & benign_samples\n",
    "print(f\"Intersection of Adversarial and Benign Samples: {len(intersection)}\")\n",
    "\n",
    "# find incides from class [0, 0, 1, 0] of y_pred_indices\n",
    "normal_indices = np.where(y_pred_indices == 2)[0]\n",
    "print(f\"Normal Samples: {len(normal_indices)} | {normal_indices[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Manual Evaluation\n",
    "We perform the whole two-stages approach on new unseen data and evaluate the following scores:\n",
    "- Recall\n",
    "- Precision\n",
    "- Accuracy\n",
    "- F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Preprocessing data --\n",
      "--- Sampling balanced data ---\n",
      "Sample to shape: (1000, 79)\n",
      "--- Splitting labels and features ---\n",
      "--- Encoding labels as binary one-hot values ---\n",
      "--- Normalizing features using MinMaxScaler ---\n",
      "Generate Features | Indices: Index([2056787, 2391506, 802264, 1981689, 480604], dtype='int64')... | Shape: (1000, 70)\n",
      "Generate Labels | Indices: Index([2056787, 2391506, 802264, 1981689, 480604], dtype='int64')... | Shape: (1000, 2)\n",
      "BENIGN  ATTACK\n",
      "False   True      500\n",
      "True    False     500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import functions.data_preprocessing as dp\n",
    "import importlib\n",
    "importlib.reload(dp)\n",
    "\n",
    "# exclude previously used samples\n",
    "dataset_eval_excluded = dataset.drop(index=used_indices)\n",
    "\n",
    "X_eval, y_eval, used_eval_indices = dp.preprocess_data(dataset_eval_excluded, encoding_type, normalizer, zero_columns, sample_size=500, random_sample_state=17)\n",
    "print(f\"Generate Features | Indices: {X_eval.index[:5]}... | Shape: {X_eval.shape}\")\n",
    "print(f\"Generate Labels | Indices: {y_eval.index[:5]}... | Shape: {y_eval.shape}\")\n",
    "print(y_eval.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial FGSM examples generated. Shape: (1000, 70)\n",
      "Create Adversarial Attack | Indices: Index([2056787, 2391506, 802264, 1981689, 480604], dtype='int64')... | Shape: (1000, 70)\n",
      "Accuracy: 50.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ATTACK       0.00      0.00      0.00       500\n",
      "      BENIGN       0.50      1.00      0.67       500\n",
      "\n",
      "    accuracy                           0.50      1000\n",
      "   macro avg       0.25      0.50      0.33      1000\n",
      "weighted avg       0.25      0.50      0.33      1000\n",
      "\n",
      "Confusion Matrix: Positive == BENIGN\n",
      "TN: 0, FP: 500, FN: 0, TP: 500\n",
      "Predictions on Adversarial Attacks | Indices: Index([2056787, 2391506, 802264, 1981689, 480604], dtype='int64')... | Shape: (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(ag)\n",
    "\n",
    "# X_adv_eval = ag.generate_cw_attacks_parallel(art_model, X_eval, target_label=1, num_cores=num_cores)\n",
    "# print(f\"Create Adversarial Attack | Indices: {X_adv_eval.index[:5]}... | Shape: {X_adv_eval.shape}\")\n",
    "\n",
    "X_adv_eval = ag.generate_fgsm_attacks(art_model, X_eval, target_label=1)\n",
    "print(f\"Create Adversarial Attack | Indices: {X_adv_eval.index[:5]}... | Shape: {X_adv_eval.shape}\")\n",
    "\n",
    "y_pred_adv_eval = ag.evaluate_art_model(art_model, X_adv_eval, y_eval)\n",
    "print(f\"Predictions on Adversarial Attacks | Indices: {y_pred_adv_eval.index[:5]}... | Shape: {y_pred_adv_eval.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 1001it [00:29, 22.27it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Explanations | Indices: Index([2056787, 2391506, 802264, 1981689, 480604], dtype='int64')... | Shape: (1000, 70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(exp)\n",
    "X_eval_adv_shap_values, X_eval_adv_shap_values_df = exp.generate_shap_values(explainer, X_adv_eval)\n",
    "\n",
    "print(f\"Create Explanations | Indices: {X_eval_adv_shap_values_df.index[:5]}... | Shape: {X_eval_adv_shap_values_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/32\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 12:20:04.257927: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    }
   ],
   "source": [
    "benign_eval_idx = y_eval[y_eval['BENIGN'] == 1].index\n",
    "attack_eval_idx = y_eval[y_eval['ATTACK'] == 1].index\n",
    "\n",
    "pred_benign_idx = y_pred_adv_eval[y_pred_adv_eval['BENIGN'] == 1].index\n",
    "pred_attack_idx = y_pred_adv_eval[y_pred_adv_eval['ATTACK'] == 1].index\n",
    "\n",
    "# predict\n",
    "X_eval_detector = X_eval_adv_shap_values_df.loc[pred_benign_idx]\n",
    "\n",
    "y_pred_eval_detector = det.predict(detector, X_eval_detector, y_eval.columns)\n",
    "\n",
    "correctly_classified_det_idx = y_pred_eval_detector[y_pred_eval_detector['BENIGN'] == 1].index\n",
    "misclassified_det_idx = y_pred_eval_detector[y_pred_eval_detector['ATTACK'] == 1].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='int64')\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step\n",
      "Index([], dtype='int64')\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "detector_misclassifies_attack_as_correct_benign = attack_eval_idx.intersection(correctly_classified_det_idx)\n",
    "print(detector_misclassifies_attack_as_correct_benign)\n",
    "\n",
    "y_pred_test = detector.predict(X_eval_detector)\n",
    "y_pred_test = pd.DataFrame(y_pred_test, index=X_eval_detector.index, columns=y_eval.columns)\n",
    "\n",
    "y_pred_test_index = y_pred_test[y_pred_test['BENIGN'] >= 0.8].index\n",
    "\n",
    "print(detector_misclassifies_attack_as_correct_benign.intersection(y_pred_test_index))\n",
    "print(len(detector_misclassifies_attack_as_correct_benign.intersection(y_pred_test_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDS classifies 'ATTACK' samples as 'ATTACK': 0\n",
      "IDS mis-classifies 'BENIGN' samples as 'ATTACK': 0\n",
      "Detector classifies 'BENIGN' samples as correct 'BENIGN': 43\n",
      "Detector mis-classifies 'ATTACK' samples as correct 'BENIGN': 0\n",
      "Detector classifies 'ATTACK' samples as misclassified due to 'ATTACK': 500\n",
      "Detector classifies 'BENIGN' samples as misclassified due to 'ATTACK': 457\n",
      "TP: 43\n",
      "FP: 0\n",
      "TN: 500\n",
      "FN: 457\n",
      "Sum: 1000\n"
     ]
    }
   ],
   "source": [
    "# After IDS Stage\n",
    "TN = len(attack_eval_idx.intersection(pred_attack_idx)) # IDS classifies 'ATTACK' samples as 'ATTACK'\n",
    "print(f\"IDS classifies 'ATTACK' samples as 'ATTACK': {TN}\")\n",
    "FN = len(benign_eval_idx.intersection(pred_attack_idx)) # IDS classifies 'BENIGN' samples as 'ATTACK'\n",
    "print(f\"IDS mis-classifies 'BENIGN' samples as 'ATTACK': {FN}\")\n",
    "\n",
    "# After Detector Stage\n",
    "TP = len(benign_eval_idx.intersection(correctly_classified_det_idx)) # Detector classifies 'BENIGN' samples as correct 'BENIGN'\n",
    "print(f\"Detector classifies 'BENIGN' samples as correct 'BENIGN': {TP}\")\n",
    "FP = len(attack_eval_idx.intersection(correctly_classified_det_idx)) # Detector classifies 'ATTACK' samples as correct 'BENIGN'\n",
    "print(f\"Detector mis-classifies 'ATTACK' samples as correct 'BENIGN': {FP}\")\n",
    "\n",
    "TN_2 = len(attack_eval_idx.intersection(misclassified_det_idx)) # Detector classifies 'ATTACK' samples as misclassified due to 'ATTACK'\n",
    "print(f\"Detector classifies 'ATTACK' samples as misclassified due to 'ATTACK': {TN_2}\")\n",
    "FN_2 = len(benign_eval_idx.intersection(misclassified_det_idx)) # Detector classifies 'BENIGN' samples as misclassified due to 'ATTACK'\n",
    "print(f\"Detector classifies 'BENIGN' samples as misclassified due to 'ATTACK': {FN_2}\")\n",
    "\n",
    "# Sum up TN & FN from both stages\n",
    "TN = TN + TN_2\n",
    "FN = FN + FN_2\n",
    "\n",
    "print(f\"TP: {TP}\")\n",
    "print(f\"FP: {FP}\")\n",
    "print(f\"TN: {TN}\")\n",
    "print(f\"FN: {FN}\")\n",
    "print(f\"Sum: {TP + FP + TN + FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Accuracy: 96.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ATTACK       0.95      0.98      0.96       500\n",
      "      BENIGN       0.98      0.94      0.96       500\n",
      "\n",
      "    accuracy                           0.96      1000\n",
      "   macro avg       0.96      0.96      0.96      1000\n",
      "weighted avg       0.96      0.96      0.96      1000\n",
      "\n",
      "True Negative Rate: 97.60%\n",
      "False Positive Rate: 2.40%\n",
      "True Positive Rate: 94.40%\n",
      "False Negative Rate: 5.60%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(f\"Global Accuracy: {(TP + TN) / (TP + FP + TN + FN) * 100:.2f}%\")\n",
    "\n",
    "# Construct a fake y_true and y_pred to match sklearn's classification_report format\n",
    "y_true = np.array([1] * TP + [0] * TN + [1] * FN + [0] * FP)  # True labels\n",
    "y_pred = np.array([1] * TP + [0] * TN + [0] * FN + [1] * FP)  # Predicted labels\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_true, y_pred, target_names=['ATTACK', 'BENIGN']) # reverse labels because classification_report assumes first label is 0\n",
    "print(report)\n",
    "\n",
    "print(f\"True Negative Rate: {TN/(TN+FP)*100:.2f}%\")\n",
    "print(f\"False Positive Rate: {FP/(TN+FP)*100:.2f}%\")\n",
    "print(f\"True Positive Rate: {TP/(TP+FN)*100:.2f}%\")\n",
    "print(f\"False Negative Rate: {FN/(TP+FN)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
