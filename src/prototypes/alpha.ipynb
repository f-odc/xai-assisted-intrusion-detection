{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype: Alpha\n",
    "\n",
    "| Properties      | Data    |\n",
    "|---------------|-----------|\n",
    "| *Labels* | `['BENIGN', 'DDoS']` |\n",
    "| *Normalization* | `Min-Max` |\n",
    "| *Sample Size* | `2000`|\n",
    "| *Adversarial Attack* | `FGSM` |\n",
    "| *Explanations* | `SHAP` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To import modules from the functions directory\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Building CICIDS2017 dataset --\n",
      "--- Combining all CICIDS2017 files ---\n",
      "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "Monday-WorkingHours.pcap_ISCX.csv\n",
      "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "Wednesday-workingHours.pcap_ISCX.csv\n",
      "--- Removing NaN and Infinity values ---\n",
      "Removing 1358 Rows with NaN values\n",
      "Removing 1509 Rows with Infinity values\n",
      "--- Extracting labels ---\n",
      " Label\n",
      "BENIGN    2271320\n",
      "DDoS       128025\n",
      "Name: count, dtype: int64\n",
      "-- Generating normalizer --\n",
      "--- Splitting labels and features ---\n",
      "Zero Columns: [' Bwd PSH Flags', ' Bwd URG Flags', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate']\n",
      "-- Preprocessing data --\n",
      "--- Sampling balanced data ---\n",
      "Sample to shape: (2000, 79)\n",
      "--- Splitting labels and features ---\n",
      "--- Encoding labels as binary one-hot values ---\n",
      "--- Normalizing features using MinMaxScaler ---\n",
      "BENIGN  ATTACK\n",
      "False   True      1000\n",
      "True    False     1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import functions.data_preprocessing as dp\n",
    "import importlib\n",
    "importlib.reload(dp)\n",
    "\n",
    "encoding_type = 0 # binary encoding\n",
    "norm_type = 0 # min-max normalization\n",
    "label_names = ['BENIGN', 'DDoS'] # labels to include\n",
    "sample_size = 1000 # sample size for each label\n",
    "\n",
    "dataset = dp.build_dataset(label_names)\n",
    "\n",
    "normalizer, zero_columns = dp.generate_normalizer(dataset, norm_type)\n",
    "\n",
    "feature_df, label_df = dp.preprocess_data(dataset, encoding_type, normalizer, zero_columns, sample_size, 42)\n",
    "print(label_df.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 70) (400, 70) (1600, 2) (400, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, label_df, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5798 - loss: 0.6775 - val_accuracy: 0.8500 - val_loss: 0.6123\n",
      "Epoch 2/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8842 - loss: 0.5952 - val_accuracy: 0.8656 - val_loss: 0.5244\n",
      "Epoch 3/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9131 - loss: 0.4876 - val_accuracy: 0.9625 - val_loss: 0.3888\n",
      "Epoch 4/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9822 - loss: 0.3422 - val_accuracy: 0.9688 - val_loss: 0.2621\n",
      "Epoch 5/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9789 - loss: 0.2168 - val_accuracy: 0.9656 - val_loss: 0.1747\n",
      "Epoch 6/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9720 - loss: 0.1459 - val_accuracy: 0.9656 - val_loss: 0.1313\n",
      "Epoch 7/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9705 - loss: 0.1072 - val_accuracy: 0.9656 - val_loss: 0.1108\n",
      "Epoch 8/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9763 - loss: 0.0813 - val_accuracy: 0.9688 - val_loss: 0.0999\n",
      "Epoch 9/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9785 - loss: 0.0678 - val_accuracy: 0.9688 - val_loss: 0.0946\n",
      "Epoch 10/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9824 - loss: 0.0593 - val_accuracy: 0.9688 - val_loss: 0.0907\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Global Accuracy: 97.50%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      0.95      0.97       199\n",
      "      ATTACK       0.95      1.00      0.98       201\n",
      "\n",
      "    accuracy                           0.97       400\n",
      "   macro avg       0.98      0.97      0.97       400\n",
      "weighted avg       0.98      0.97      0.97       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import functions.intrusion_detection_system as ids\n",
    "import importlib\n",
    "importlib.reload(ids)\n",
    "\n",
    "ids_model = ids.build_intrusion_detection_system(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial FGSM examples generated. Shape: (1600, 70)\n",
      "   Destination Port  Flow Duration  Total Fwd Packets\n",
      "0          0.790196            0.1                0.0\n",
      "1          0.000000            0.1                0.0\n",
      "Accuracy: 0.271875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       0.26      0.24      0.25       801\n",
      "      ATTACK       0.28      0.30      0.29       799\n",
      "\n",
      "   micro avg       0.27      0.27      0.27      1600\n",
      "   macro avg       0.27      0.27      0.27      1600\n",
      "weighted avg       0.27      0.27      0.27      1600\n",
      " samples avg       0.27      0.27      0.27      1600\n",
      "\n",
      "Confusion Matrix: Positive == BENIGN\n",
      "TN: 241, FP: 558, FN: 607, TP: 194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.271875"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functions.attack_generator as ag\n",
    "import importlib\n",
    "importlib.reload(ag)\n",
    "\n",
    "all_features = dataset.drop(columns=[' Label'])\n",
    "art_model = ag.convert_to_art_model(ids_model, all_features)\n",
    "\n",
    "# import numpy as np\n",
    "# target_label = np.zeros_like(y_train)\n",
    "# target_label[:, 0] = 1 # desired predicted label = [1, 0] = BENIGN\n",
    "# print(target_label[:3])\n",
    "\n",
    "X_adv_fgsm = ag.generate_fgsm_attacks(art_model, X_train)\n",
    "\n",
    "import pandas as pd\n",
    "X_adv_fgsm_df = pd.DataFrame(X_adv_fgsm, columns=X_train.columns)\n",
    "print(X_adv_fgsm_df.iloc[:, :3].head(2))\n",
    "ag.evaluate_art_model(art_model, X_adv_fgsm, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 1601it [02:00, 12.22it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 1601it [01:43, 13.99it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 70)\n"
     ]
    }
   ],
   "source": [
    "import functions.explainer as exp\n",
    "import importlib\n",
    "importlib.reload(exp)\n",
    "\n",
    "explainer = exp.generate_shap_explainer(ids_model, X_train)\n",
    "\n",
    "shap_values = exp.generate_shap_values(explainer, X_train)\n",
    "print(shap_values.shape)\n",
    "shap_values_df = exp.convert_shap_values_to_pd(shap_values, X_train.columns)\n",
    "\n",
    "shap_values_adv = exp.generate_shap_values(explainer, X_adv_fgsm)\n",
    "print(shap_values_adv.shape)\n",
    "shap_values_adv_df = exp.convert_shap_values_to_pd(shap_values_adv, X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 70) (3200, 2)\n",
      "(2880, 70) (320, 70) (2880, 2) (320, 2)\n",
      "Epoch 1/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5246 - loss: 0.6903 - val_accuracy: 0.5122 - val_loss: 0.6842\n",
      "Epoch 2/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5134 - loss: 0.6809 - val_accuracy: 0.5451 - val_loss: 0.6663\n",
      "Epoch 3/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5964 - loss: 0.6578 - val_accuracy: 0.6354 - val_loss: 0.6248\n",
      "Epoch 4/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7346 - loss: 0.6108 - val_accuracy: 0.8021 - val_loss: 0.5528\n",
      "Epoch 5/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8240 - loss: 0.5387 - val_accuracy: 0.9028 - val_loss: 0.4713\n",
      "Epoch 6/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9011 - loss: 0.4689 - val_accuracy: 0.9358 - val_loss: 0.4141\n",
      "Epoch 7/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9349 - loss: 0.4211 - val_accuracy: 0.9549 - val_loss: 0.3774\n",
      "Epoch 8/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9536 - loss: 0.3781 - val_accuracy: 0.9653 - val_loss: 0.3528\n",
      "Epoch 9/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9604 - loss: 0.3628 - val_accuracy: 0.9635 - val_loss: 0.3346\n",
      "Epoch 10/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9723 - loss: 0.3313 - val_accuracy: 0.9740 - val_loss: 0.3205\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Global Accuracy: 98.44%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       0.98      0.99      0.98       149\n",
      " ADVERSARIAL       0.99      0.98      0.99       171\n",
      "\n",
      "    accuracy                           0.98       320\n",
      "   macro avg       0.98      0.98      0.98       320\n",
      "weighted avg       0.98      0.98      0.98       320\n",
      "\n",
      "True Negative Rate: 98.66%\n",
      "False Positive Rate: 1.34%\n",
      "True Positive Rate: 98.25%\n",
      "False Negative Rate: 1.75%\n"
     ]
    }
   ],
   "source": [
    "import functions.detector as det\n",
    "import importlib\n",
    "importlib.reload(det)\n",
    "import numpy as np\n",
    "\n",
    "# create dataframe\n",
    "X, y = det.build_train_datasets(shap_values_df, shap_values_adv_df)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# create normalizer\n",
    "# normalizer = det.create_min_max_normalizer(X)\n",
    "# print(np.max(normalizer.data_max_))\n",
    "# print(np.min(normalizer.data_min_))\n",
    "\n",
    "# TODO: how to normalize? min/max should be consistent for all datasets\n",
    "# normalize features\n",
    "# X = normalizer.transform(X)\n",
    "# print(np.max(X))\n",
    "# print(np.min(X))\n",
    "\n",
    "# split data\n",
    "X_train_det, X_test_det, y_train_det, y_test_det = train_test_split(X, y, test_size=0.1, random_state=1503)\n",
    "print(X_train_det.shape, X_test_det.shape, y_train_det.shape, y_test_det.shape)\n",
    "\n",
    "# build detector\n",
    "detector = det.build_detector(X_train_det, y_train_det, X_test_det, y_test_det)\n",
    "\n",
    "\n",
    "# normalizer_adv = det.create_min_max_normalizer(shap_values_adv_df)\n",
    "# print(np.max(normalizer_adv.data_max_))\n",
    "# print(np.min(normalizer_adv.data_min_))\n",
    "\n",
    "# shap_values_normalized = det.normalize_shap_values(shap_values_df)\n",
    "# print(shap_values_normalized.shape)\n",
    "# print(np.max(shap_values_normalized))\n",
    "# print(np.min(shap_values_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 70)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial FGSM examples generated. Shape: (400, 70)\n"
     ]
    }
   ],
   "source": [
    "X_test_adv = ag.generate_fgsm_attacks(art_model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 401it [00:26,  9.47it/s]                         \n",
      "PermutationExplainer explainer: 401it [00:30,  8.75it/s]                         \n"
     ]
    }
   ],
   "source": [
    "X_test_adv_shap_values = exp.generate_shap_values(explainer, X_test_adv)\n",
    "X_test_shap_values = exp.generate_shap_values(explainer, X_test)\n",
    "X_test_shap_values_df = exp.convert_shap_values_to_pd(X_test_shap_values, X_test.columns)\n",
    "X_test_adv_shap_values_df = exp.convert_shap_values_to_pd(X_test_adv_shap_values, X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = det.build_train_datasets(X_test_shap_values_df, X_test_adv_shap_values_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.max(X))\n",
    "# print(np.min(X))\n",
    "# X = normalizer.transform(X)\n",
    "# print(np.max(X))\n",
    "# print(np.min(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle Data & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X, y = shuffle(X, y, random_state=187)\n",
    "\n",
    "y_pred = detector.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Accuracy: 97.88%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       0.97      0.99      0.98       400\n",
      " ADVERSARIAL       0.99      0.97      0.98       400\n",
      "\n",
      "    accuracy                           0.98       800\n",
      "   macro avg       0.98      0.98      0.98       800\n",
      "weighted avg       0.98      0.98      0.98       800\n",
      "\n",
      "True Negative Rate: 98.75%\n",
      "False Positive Rate: 1.25%\n",
      "True Positive Rate: 97.00%\n",
      "False Negative Rate: 3.00%\n"
     ]
    }
   ],
   "source": [
    "det.evaluate_model(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Additional Evaluation - New Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Removing NaN and Infinity values ---\n",
      "Number of rows with NaN values:  0\n",
      "Removing NaN values....\n",
      "Number of rows with Infinity values: 0\n",
      "Removing Infinity values....\n",
      "--- Sampling balanced data ---\n",
      "Sample to shape: (500, 79)\n",
      "--- Splitting labels and features ---\n",
      "--- Encoding labels as binary one-hot values ---\n",
      "--- Normalizing features using MinMaxScaler ---\n",
      "BENIGN  ATTACK\n",
      "False   True      250\n",
      "True    False     250\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import functions.data_preprocessing as dp\n",
    "import importlib\n",
    "importlib.reload(dp)\n",
    "\n",
    "X_eval, y_eval = dp.preprocess_data(dataset, encoding_type, normalizer, zero_columns, 250, 187)\n",
    "print(y_eval.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial FGSM examples generated. Shape: (500, 70)\n",
      "   Destination Port  Flow Duration  Total Fwd Packets\n",
      "0          0.000000       0.100411                0.0\n",
      "1          0.734119       0.861346                0.0\n",
      "Accuracy: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       0.25      0.21      0.23       250\n",
      "      ATTACK       0.33      0.39      0.36       250\n",
      "\n",
      "   micro avg       0.30      0.30      0.30       500\n",
      "   macro avg       0.29      0.30      0.29       500\n",
      "weighted avg       0.29      0.30      0.29       500\n",
      " samples avg       0.30      0.30      0.30       500\n",
      "\n",
      "Confusion Matrix: Positive == BENIGN\n",
      "TN: 98, FP: 152, FN: 198, TP: 52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_adv_fgsm_eval = ag.generate_fgsm_attacks(art_model, X_eval)\n",
    "X_adv_fgsm_df = pd.DataFrame(X_adv_fgsm_eval, columns=X_eval.columns)\n",
    "print(X_adv_fgsm_df.iloc[:, :3].head(2))\n",
    "ag.evaluate_art_model(art_model, X_adv_fgsm_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 501it [00:32, 11.40it/s]                         \n",
      "PermutationExplainer explainer: 501it [00:35, 10.17it/s]                         \n"
     ]
    }
   ],
   "source": [
    "X_eval_adv_shap_values = exp.generate_shap_values(explainer, X_adv_fgsm_eval)\n",
    "X_eval_shap_values = exp.generate_shap_values(explainer, X_eval)\n",
    "X_eval_shap_values_df = exp.convert_shap_values_to_pd(X_eval_shap_values, X_eval.columns)\n",
    "X_eval_adv_shap_values_df = exp.convert_shap_values_to_pd(X_eval_adv_shap_values, X_eval.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 70) (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_eval_detector, y_eval_detector = det.build_train_datasets(X_eval_shap_values_df, X_eval_adv_shap_values_df)\n",
    "print(X_eval_detector.shape, y_eval_detector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_eval_detector, y_eval_detector = shuffle(X_eval_detector, y_eval_detector, random_state=187)\n",
    "\n",
    "y_pred_eval_detector = detector.predict(X_eval_detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Accuracy: 97.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       0.96      0.98      0.97       500\n",
      " ADVERSARIAL       0.98      0.96      0.97       500\n",
      "\n",
      "    accuracy                           0.97      1000\n",
      "   macro avg       0.97      0.97      0.97      1000\n",
      "weighted avg       0.97      0.97      0.97      1000\n",
      "\n",
      "True Negative Rate: 97.60%\n",
      "False Positive Rate: 2.40%\n",
      "True Positive Rate: 96.40%\n",
      "False Negative Rate: 3.60%\n"
     ]
    }
   ],
   "source": [
    "det.evaluate_model(y_pred_eval_detector, y_eval_detector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
